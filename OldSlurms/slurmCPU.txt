[0] setting up environment
2019-05-06 16:16:27.796266: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-05-06 16:16:27.809260: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100060000 Hz
2019-05-06 16:16:27.814400: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7fc6b3f71e20 executing computations on platform Host. Devices:
2019-05-06 16:16:27.814452: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-05-06 16:16:27.880501: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2019-05-06 16:16:27.880580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: cp1-p
2019-05-06 16:16:27.880600: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: cp1-p
2019-05-06 16:16:27.880678: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 418.40.4
2019-05-06 16:16:27.880753: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 410.72.0
2019-05-06 16:16:27.880771: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:306] kernel version 410.72.0 does not match DSO version 418.40.4 -- cannot find working devices in this configuration
2019-05-06 16:16:27.886393: I tensorflow/core/common_runtime/direct_session.cc:317] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device

Using TensorFlow backend.
WARNING:tensorflow:From /homes/nramachandra/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Cl_extendedP10.py:350: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  validation_data=(x_test_noisy, x_test), callbacks=callbacks_list)
WARNING:tensorflow:From /homes/nramachandra/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~
=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~
(1000, 9999) train sequences
(32, 9999) test sequences
(1000, 10) train sequences
(32, 10) test sequences
-------mean factor: [ 0.          0.          0.         ... -0.27508546 -0.30671477
 -0.33611096]
-------normalization factor: [1.14641721e+06 1.25621889e+06 1.32794218e+06 ... 6.39498020e-01
 6.58649880e-01 6.76453256e-01]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 9999)         0                                            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2048)         20480000    input_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         2098176     dense_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 512)          524800      dense_2[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 256)          131328      dense_3[0][0]                    
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 128)          32896       dense_4[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 64)           8256        dense_5[0][0]                    
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 64)           8256        dense_5[0][0]                    
__________________________________________________________________________________________________
lambda_1 (Lambda)               (8, 64)              0           dense_6[0][0]                    
                                                                 dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 multiple             4160        lambda_1[0][0]                   
__________________________________________________________________________________________________
dense_9 (Dense)                 multiple             8320        dense_8[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                multiple             33024       dense_9[0][0]                    
__________________________________________________________________________________________________
dense_11 (Dense)                multiple             131584      dense_10[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                multiple             525312      dense_11[0][0]                   
__________________________________________________________________________________________________
dense_13 (Dense)                multiple             2099200     dense_12[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                multiple             20487951    dense_13[0][0]                   
==================================================================================================
Total params: 46,573,263
Trainable params: 46,573,263
Non-trainable params: 0
__________________________________________________________________________________________________
None
Train on 1000 samples, validate on 32 samples
Epoch 1/1000
 - 29s - loss: 7538.7353 - val_loss: 564.6167
Epoch 2/1000
 - 28s - loss: 708.5403 - val_loss: 527.4187
Epoch 3/1000
 - 28s - loss: 698.1857 - val_loss: 526.0503
Epoch 4/1000
 - 28s - loss: 697.4765 - val_loss: 525.2507

Epoch 00004: loss improved from inf to 697.47651, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 5/1000
 - 28s - loss: 697.0874 - val_loss: 525.5281
Epoch 6/1000
 - 28s - loss: 691.6895 - val_loss: 520.2566
Epoch 7/1000
 - 28s - loss: 693.2486 - val_loss: 523.7603
Epoch 8/1000
 - 28s - loss: 691.8132 - val_loss: 520.3575

Epoch 00008: loss improved from 697.47651 to 691.81325, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 9/1000
 - 28s - loss: 690.3314 - val_loss: 519.9330
Epoch 10/1000
 - 28s - loss: 690.4560 - val_loss: 520.1153
Epoch 11/1000
 - 28s - loss: 692.8597 - val_loss: 520.4621
Epoch 12/1000
 - 28s - loss: 690.5950 - val_loss: 519.8869

Epoch 00012: loss improved from 691.81325 to 690.59496, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 13/1000
 - 28s - loss: 690.1098 - val_loss: 519.8447
Epoch 14/1000
 - 28s - loss: 690.0411 - val_loss: 519.8228
Epoch 15/1000
 - 28s - loss: 691.9583 - val_loss: 519.9776
Epoch 16/1000
 - 28s - loss: 690.1114 - val_loss: 519.8319

Epoch 00016: loss improved from 690.59496 to 690.11138, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 17/1000
 - 28s - loss: 691.7612 - val_loss: 519.7994
Epoch 18/1000
 - 27s - loss: 690.1979 - val_loss: 520.0089
Epoch 19/1000
 - 27s - loss: 689.8946 - val_loss: 519.6451
Epoch 20/1000
 - 27s - loss: 691.2256 - val_loss: 520.5082

Epoch 00020: loss did not improve from 690.11138
Epoch 21/1000
 - 27s - loss: 692.1493 - val_loss: 519.7148
Epoch 22/1000
 - 28s - loss: 690.9927 - val_loss: 519.7833
Epoch 23/1000
 - 27s - loss: 689.8719 - val_loss: 519.6766
Epoch 24/1000
 - 27s - loss: 689.8207 - val_loss: 519.6348

Epoch 00024: loss improved from 690.11138 to 689.82072, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 25/1000
 - 27s - loss: 689.7853 - val_loss: 519.6228
Epoch 26/1000
 - 27s - loss: 689.8785 - val_loss: 519.6653
Epoch 27/1000
 - 27s - loss: 694.0525 - val_loss: 520.2660
Epoch 28/1000
 - 27s - loss: 690.3873 - val_loss: 519.6841

Epoch 00028: loss did not improve from 689.82072
Epoch 29/1000
 - 28s - loss: 689.8343 - val_loss: 519.6348
Epoch 30/1000
 - 28s - loss: 690.7433 - val_loss: 520.0658
Epoch 31/1000
 - 28s - loss: 692.8860 - val_loss: 519.7107
Epoch 32/1000
 - 28s - loss: 689.9019 - val_loss: 519.6305

Epoch 00032: loss did not improve from 689.82072
Epoch 33/1000
 - 28s - loss: 689.8536 - val_loss: 519.8995
Epoch 34/1000
 - 28s - loss: 692.0910 - val_loss: 519.8854
Epoch 35/1000
 - 28s - loss: 690.2585 - val_loss: 519.7955
Epoch 36/1000
 - 28s - loss: 691.8048 - val_loss: 520.2494

Epoch 00036: loss did not improve from 689.82072
Epoch 37/1000
 - 28s - loss: 690.5074 - val_loss: 519.9472
Epoch 38/1000
 - 28s - loss: 692.4114 - val_loss: 520.1961
Epoch 39/1000
 - 28s - loss: 707.8042 - val_loss: 519.8347
Epoch 40/1000
 - 28s - loss: 690.5593 - val_loss: 519.8615

Epoch 00040: loss did not improve from 689.82072
Epoch 41/1000
 - 28s - loss: 690.0392 - val_loss: 519.8258
Epoch 42/1000
 - 28s - loss: 689.9713 - val_loss: 519.8407
Epoch 43/1000
 - 28s - loss: 689.9563 - val_loss: 519.8276
Epoch 44/1000
 - 28s - loss: 690.3432 - val_loss: 519.7024

Epoch 00044: loss did not improve from 689.82072
Epoch 45/1000
 - 28s - loss: 689.9224 - val_loss: 519.6325
Epoch 46/1000
 - 28s - loss: 689.8026 - val_loss: 519.6169
Epoch 47/1000
 - 28s - loss: 689.7848 - val_loss: 519.6181
Epoch 48/1000
 - 28s - loss: 689.7806 - val_loss: 519.6152

Epoch 00048: loss improved from 689.82072 to 689.78058, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 49/1000
 - 28s - loss: 689.7693 - val_loss: 519.5959
Epoch 50/1000
 - 28s - loss: 689.7584 - val_loss: 519.5900
Epoch 51/1000
 - 28s - loss: 689.7509 - val_loss: 519.5985
Epoch 52/1000
 - 28s - loss: 689.7526 - val_loss: 519.6129

Epoch 00052: loss improved from 689.78058 to 689.75263, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 53/1000
 - 28s - loss: 689.7507 - val_loss: 519.5892
Epoch 54/1000
 - 28s - loss: 689.7518 - val_loss: 519.5856
Epoch 55/1000
 - 28s - loss: 689.7711 - val_loss: 519.6125
Epoch 56/1000
 - 28s - loss: 689.7650 - val_loss: 519.6259

Epoch 00056: loss did not improve from 689.75263
Epoch 57/1000
 - 28s - loss: 689.7627 - val_loss: 519.6839
Epoch 58/1000
 - 28s - loss: 689.6682 - val_loss: 519.6327
Epoch 59/1000
 - 28s - loss: 690.0081 - val_loss: 519.7777
Epoch 60/1000
 - 28s - loss: 689.8515 - val_loss: 519.6346

Epoch 00060: loss did not improve from 689.75263
Epoch 61/1000
 - 28s - loss: 689.7829 - val_loss: 519.6999
Epoch 62/1000
 - 28s - loss: 691.7450 - val_loss: 519.8293
Epoch 63/1000
 - 28s - loss: 689.9377 - val_loss: 519.7624
Epoch 64/1000
 - 28s - loss: 689.8595 - val_loss: 519.6784

Epoch 00064: loss did not improve from 689.75263
Epoch 65/1000
 - 28s - loss: 689.8143 - val_loss: 519.6500
Epoch 66/1000
 - 28s - loss: 689.8028 - val_loss: 519.6234
Epoch 67/1000
 - 28s - loss: 689.7822 - val_loss: 519.6890
Epoch 68/1000
 - 28s - loss: 689.7972 - val_loss: 519.6005

Epoch 00068: loss did not improve from 689.75263
Epoch 69/1000
 - 28s - loss: 689.7777 - val_loss: 519.6059
Epoch 70/1000
 - 28s - loss: 689.7637 - val_loss: 519.6249
Epoch 71/1000
 - 28s - loss: 689.7866 - val_loss: 519.5928
Epoch 72/1000
 - 28s - loss: 689.7802 - val_loss: 519.5890

Epoch 00072: loss did not improve from 689.75263
Epoch 73/1000
 - 28s - loss: 689.7607 - val_loss: 519.5933
Epoch 74/1000
 - 28s - loss: 689.7488 - val_loss: 519.5817
Epoch 75/1000
 - 28s - loss: 689.7558 - val_loss: 519.5975
Epoch 76/1000
 - 28s - loss: 689.7549 - val_loss: 519.5954

Epoch 00076: loss did not improve from 689.75263
Epoch 77/1000
 - 28s - loss: 689.7423 - val_loss: 519.5903
Epoch 78/1000
 - 28s - loss: 689.7489 - val_loss: 519.5752
Epoch 79/1000
 - 28s - loss: 689.7566 - val_loss: 519.7836
Epoch 80/1000
 - 28s - loss: 689.7440 - val_loss: 519.5700

Epoch 00080: loss improved from 689.75263 to 689.74398, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 81/1000
 - 28s - loss: 689.7354 - val_loss: 519.5777
Epoch 82/1000
 - 27s - loss: 689.8505 - val_loss: 519.7337
Epoch 83/1000
 - 28s - loss: 689.8894 - val_loss: 519.6021
Epoch 84/1000
 - 28s - loss: 689.7546 - val_loss: 519.5835

Epoch 00084: loss did not improve from 689.74398
Epoch 85/1000
 - 28s - loss: 690.1758 - val_loss: 526.4989
Epoch 86/1000
 - 28s - loss: 693.1460 - val_loss: 521.7300
Epoch 87/1000
 - 28s - loss: 692.1011 - val_loss: 521.6970
Epoch 88/1000
 - 28s - loss: 692.0600 - val_loss: 521.6946

Epoch 00088: loss did not improve from 689.74398
Epoch 89/1000
 - 28s - loss: 693.3380 - val_loss: 522.0334
Epoch 90/1000
 - 27s - loss: 1677.6427 - val_loss: 521.8334
Epoch 91/1000
 - 27s - loss: 692.2399 - val_loss: 521.7442
Epoch 92/1000
 - 28s - loss: 692.1386 - val_loss: 521.8453

Epoch 00092: loss did not improve from 689.74398
Epoch 93/1000
 - 28s - loss: 692.0988 - val_loss: 521.7055
Epoch 94/1000
 - 28s - loss: 692.0699 - val_loss: 521.7526
Epoch 95/1000
 - 28s - loss: 692.0478 - val_loss: 521.8217
Epoch 96/1000
 - 28s - loss: 692.8173 - val_loss: 521.8381

Epoch 00096: loss did not improve from 689.74398
Epoch 97/1000
 - 28s - loss: 692.1048 - val_loss: 521.6459
Epoch 98/1000
 - 28s - loss: 691.8665 - val_loss: 521.4523
Epoch 99/1000
 - 28s - loss: 691.7938 - val_loss: 521.4506
Epoch 100/1000
 - 28s - loss: 691.7805 - val_loss: 521.4301

Epoch 00100: loss did not improve from 689.74398
Epoch 101/1000
 - 28s - loss: 691.7715 - val_loss: 521.4204
Epoch 102/1000
 - 28s - loss: 691.7641 - val_loss: 521.4155
Epoch 103/1000
 - 27s - loss: 691.7633 - val_loss: 521.4194
Epoch 104/1000
 - 28s - loss: 691.7677 - val_loss: 521.4160

Epoch 00104: loss did not improve from 689.74398
Epoch 105/1000
 - 28s - loss: 691.7555 - val_loss: 521.4089
Epoch 106/1000
 - 28s - loss: 691.7664 - val_loss: 521.4213
Epoch 107/1000
 - 28s - loss: 691.7517 - val_loss: 521.4105
Epoch 108/1000
 - 28s - loss: 691.7483 - val_loss: 521.4088

Epoch 00108: loss did not improve from 689.74398
Epoch 109/1000
 - 28s - loss: 691.7738 - val_loss: 521.4702
Epoch 110/1000
 - 28s - loss: 691.7697 - val_loss: 521.4455
Epoch 111/1000
 - 28s - loss: 691.7563 - val_loss: 521.4066
Epoch 112/1000
 - 28s - loss: 691.7391 - val_loss: 521.4054

Epoch 00112: loss did not improve from 689.74398
Epoch 113/1000
 - 28s - loss: 691.7373 - val_loss: 521.4215
Epoch 114/1000
 - 28s - loss: 691.7375 - val_loss: 521.4027
Epoch 115/1000
 - 28s - loss: 691.7497 - val_loss: 521.4105
Epoch 116/1000
 - 28s - loss: 691.7456 - val_loss: 521.4410

Epoch 00116: loss did not improve from 689.74398
Epoch 117/1000
 - 28s - loss: 691.7851 - val_loss: 521.4185
Epoch 118/1000
 - 28s - loss: 692.2763 - val_loss: 522.0537
Epoch 119/1000
 - 28s - loss: 692.7855 - val_loss: 521.0986
Epoch 120/1000
 - 28s - loss: 690.3927 - val_loss: 519.7672

Epoch 00120: loss did not improve from 689.74398
Epoch 121/1000
 - 27s - loss: 689.9217 - val_loss: 519.7802
Epoch 122/1000
 - 27s - loss: 689.9461 - val_loss: 519.6688
Epoch 123/1000
 - 27s - loss: 689.7593 - val_loss: 519.6649
Epoch 124/1000
 - 27s - loss: 689.7427 - val_loss: 519.6161

Epoch 00124: loss improved from 689.74398 to 689.74266, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 125/1000
 - 27s - loss: 689.7299 - val_loss: 519.6384
Epoch 126/1000
 - 28s - loss: 689.7196 - val_loss: 519.5651
Epoch 127/1000
 - 28s - loss: 689.7146 - val_loss: 519.5677
Epoch 128/1000
 - 28s - loss: 689.7244 - val_loss: 519.5646

Epoch 00128: loss improved from 689.74266 to 689.72438, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 129/1000
 - 28s - loss: 689.7100 - val_loss: 519.5873
Epoch 130/1000
 - 28s - loss: 689.7571 - val_loss: 519.5928
Epoch 131/1000
 - 28s - loss: 689.7339 - val_loss: 519.5770
Epoch 132/1000
 - 28s - loss: 689.7615 - val_loss: 519.6740

Epoch 00132: loss did not improve from 689.72438
Epoch 133/1000
 - 28s - loss: 689.7328 - val_loss: 519.5732
Epoch 134/1000
 - 28s - loss: 689.7574 - val_loss: 519.5782
Epoch 135/1000
 - 28s - loss: 689.7364 - val_loss: 519.5728
Epoch 136/1000
 - 27s - loss: 689.7146 - val_loss: 519.5713

Epoch 00136: loss improved from 689.72438 to 689.71458, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 137/1000
 - 28s - loss: 689.7444 - val_loss: 519.6300
Epoch 138/1000
 - 28s - loss: 689.9942 - val_loss: 519.6410
Epoch 139/1000
 - 28s - loss: 689.7391 - val_loss: 519.5966
Epoch 140/1000
 - 28s - loss: 689.7539 - val_loss: 519.5885

Epoch 00140: loss did not improve from 689.71458
Epoch 141/1000
 - 28s - loss: 689.7486 - val_loss: 519.6341
Epoch 142/1000
 - 28s - loss: 689.7239 - val_loss: 519.5679
Epoch 143/1000
 - 27s - loss: 689.7045 - val_loss: 519.5686
Epoch 144/1000
 - 27s - loss: 693.2477 - val_loss: 519.7608

Epoch 00144: loss did not improve from 689.71458
Epoch 145/1000
 - 28s - loss: 692.3144 - val_loss: 519.7370
Epoch 146/1000
 - 28s - loss: 690.3525 - val_loss: 523.7695
Epoch 147/1000
 - 28s - loss: 690.4413 - val_loss: 519.6524
Epoch 148/1000
 - 28s - loss: 689.7777 - val_loss: 519.6330

Epoch 00148: loss did not improve from 689.71458
Epoch 149/1000
 - 28s - loss: 689.7413 - val_loss: 519.5803
Epoch 150/1000
 - 28s - loss: 689.7271 - val_loss: 519.5767
Epoch 151/1000
 - 28s - loss: 689.7312 - val_loss: 519.5782
Epoch 152/1000
 - 28s - loss: 689.7181 - val_loss: 519.5639

Epoch 00152: loss did not improve from 689.71458
Epoch 153/1000
 - 28s - loss: 689.7414 - val_loss: 519.6374
Epoch 154/1000
 - 27s - loss: 689.7305 - val_loss: 519.5669
Epoch 155/1000
 - 27s - loss: 689.7208 - val_loss: 519.5714
Epoch 156/1000
 - 28s - loss: 689.7203 - val_loss: 519.5643

Epoch 00156: loss did not improve from 689.71458
Epoch 157/1000
 - 27s - loss: 689.7321 - val_loss: 519.5735
Epoch 158/1000
 - 28s - loss: 689.7104 - val_loss: 519.5617
Epoch 159/1000
 - 27s - loss: 689.7051 - val_loss: 519.5639
Epoch 160/1000
 - 27s - loss: 689.7158 - val_loss: 519.6561

Epoch 00160: loss did not improve from 689.71458
Epoch 161/1000
 - 28s - loss: 689.7238 - val_loss: 519.6297
Epoch 162/1000
 - 28s - loss: 689.7304 - val_loss: 519.5617
Epoch 163/1000
 - 28s - loss: 689.7062 - val_loss: 519.5625
Epoch 164/1000
 - 27s - loss: 689.7060 - val_loss: 519.5625

Epoch 00164: loss improved from 689.71458 to 689.70602, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 165/1000
 - 28s - loss: 689.7051 - val_loss: 519.5639
Epoch 166/1000
 - 28s - loss: 689.7009 - val_loss: 519.5538
Epoch 167/1000
 - 28s - loss: 689.7099 - val_loss: 519.5747
Epoch 168/1000
 - 28s - loss: 689.7116 - val_loss: 519.5593

Epoch 00168: loss did not improve from 689.70602
Epoch 169/1000
 - 28s - loss: 689.6990 - val_loss: 519.5534
Epoch 170/1000
 - 28s - loss: 689.6977 - val_loss: 519.5989
Epoch 171/1000
 - 27s - loss: 689.7425 - val_loss: 519.5660
Epoch 172/1000
 - 28s - loss: 689.6994 - val_loss: 519.5563

Epoch 00172: loss improved from 689.70602 to 689.69939, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 173/1000
 - 27s - loss: 689.7025 - val_loss: 519.5558
Epoch 174/1000
 - 28s - loss: 689.8488 - val_loss: 519.6427
Epoch 175/1000
 - 28s - loss: 690.5091 - val_loss: 519.9417
Epoch 176/1000
 - 28s - loss: 689.8922 - val_loss: 519.6113

Epoch 00176: loss did not improve from 689.69939
Epoch 177/1000
 - 28s - loss: 690.3859 - val_loss: 520.3363
Epoch 178/1000
 - 28s - loss: 689.9389 - val_loss: 519.6467
Epoch 179/1000
 - 28s - loss: 689.7890 - val_loss: 519.6096
Epoch 180/1000
 - 28s - loss: 689.7644 - val_loss: 519.6120

Epoch 00180: loss did not improve from 689.69939
Epoch 181/1000
 - 28s - loss: 689.7464 - val_loss: 519.5846
Epoch 182/1000
 - 28s - loss: 689.7372 - val_loss: 519.5772
Epoch 183/1000
 - 28s - loss: 689.7364 - val_loss: 519.5771
Epoch 184/1000
 - 28s - loss: 689.7348 - val_loss: 519.8225

Epoch 00184: loss did not improve from 689.69939
Epoch 185/1000
 - 28s - loss: 689.7451 - val_loss: 519.5679
Epoch 186/1000
 - 28s - loss: 689.7200 - val_loss: 519.5632
Epoch 187/1000
 - 28s - loss: 689.7161 - val_loss: 519.5605
Epoch 188/1000
 - 28s - loss: 689.7136 - val_loss: 519.5592

Epoch 00188: loss did not improve from 689.69939
Epoch 189/1000
 - 28s - loss: 689.7138 - val_loss: 519.5606
Epoch 190/1000
 - 28s - loss: 689.7140 - val_loss: 519.5643
Epoch 191/1000
 - 28s - loss: 689.7245 - val_loss: 519.5655
Epoch 192/1000
 - 28s - loss: 689.7101 - val_loss: 519.5639

Epoch 00192: loss did not improve from 689.69939
Epoch 193/1000
 - 28s - loss: 689.7144 - val_loss: 519.5553
Epoch 194/1000
 - 28s - loss: 689.7081 - val_loss: 519.5583
Epoch 195/1000
 - 28s - loss: 689.7063 - val_loss: 519.5552
Epoch 196/1000
 - 28s - loss: 689.7061 - val_loss: 519.5544

Epoch 00196: loss did not improve from 689.69939
Epoch 197/1000
 - 28s - loss: 689.7126 - val_loss: 519.5688
Epoch 198/1000
 - 28s - loss: 689.7094 - val_loss: 519.5534
Epoch 199/1000
 - 28s - loss: 689.7068 - val_loss: 519.5616
Epoch 200/1000
 - 28s - loss: 689.7064 - val_loss: 519.5527

Epoch 00200: loss did not improve from 689.69939
Epoch 201/1000
 - 28s - loss: 689.7050 - val_loss: 519.5537
Epoch 202/1000
 - 28s - loss: 689.7133 - val_loss: 519.5538
Epoch 203/1000
 - 28s - loss: 689.7221 - val_loss: 519.5566
Epoch 204/1000
 - 28s - loss: 689.7084 - val_loss: 519.5584

Epoch 00204: loss did not improve from 689.69939
Epoch 205/1000
 - 28s - loss: 689.7197 - val_loss: 519.5579
Epoch 206/1000
 - 28s - loss: 689.7102 - val_loss: 519.5584
Epoch 207/1000
 - 28s - loss: 689.7159 - val_loss: 519.5601
Epoch 208/1000
 - 28s - loss: 689.7211 - val_loss: 519.5591

Epoch 00208: loss did not improve from 689.69939
Epoch 209/1000
 - 28s - loss: 689.7120 - val_loss: 519.5514
Epoch 210/1000
 - 28s - loss: 689.7030 - val_loss: 519.5552
Epoch 211/1000
 - 28s - loss: 689.7103 - val_loss: 519.5512
Epoch 212/1000
 - 28s - loss: 689.7059 - val_loss: 519.5510

Epoch 00212: loss did not improve from 689.69939
Epoch 213/1000
 - 28s - loss: 689.7024 - val_loss: 519.5488
Epoch 214/1000
 - 28s - loss: 689.7053 - val_loss: 519.5530
Epoch 215/1000
 - 28s - loss: 689.7103 - val_loss: 519.5592
Epoch 216/1000
 - 28s - loss: 689.7266 - val_loss: 519.5694

Epoch 00216: loss did not improve from 689.69939
Epoch 217/1000
 - 28s - loss: 689.7135 - val_loss: 519.5523
Epoch 218/1000
 - 28s - loss: 689.7361 - val_loss: 519.5581
Epoch 219/1000
 - 28s - loss: 689.7314 - val_loss: 519.5792
Epoch 220/1000
 - 28s - loss: 689.7202 - val_loss: 519.5571

Epoch 00220: loss did not improve from 689.69939
Epoch 221/1000
 - 28s - loss: 689.7141 - val_loss: 519.5850
Epoch 222/1000
 - 28s - loss: 689.7127 - val_loss: 519.5495
Epoch 223/1000
 - 28s - loss: 689.7017 - val_loss: 519.5536
Epoch 224/1000
 - 28s - loss: 689.7035 - val_loss: 519.5659

Epoch 00224: loss did not improve from 689.69939
Epoch 225/1000
 - 28s - loss: 689.7255 - val_loss: 519.5594
Epoch 226/1000
 - 28s - loss: 689.7422 - val_loss: 519.5483
Epoch 227/1000
 - 28s - loss: 689.7119 - val_loss: 519.5583
Epoch 228/1000
 - 28s - loss: 689.7103 - val_loss: 519.5503

Epoch 00228: loss did not improve from 689.69939
Epoch 229/1000
 - 28s - loss: 689.7007 - val_loss: 519.5489
Epoch 230/1000
 - 28s - loss: 689.6997 - val_loss: 519.5499
Epoch 231/1000
 - 28s - loss: 689.6995 - val_loss: 519.5589
Epoch 232/1000
 - 28s - loss: 689.7120 - val_loss: 519.5700

Epoch 00232: loss did not improve from 689.69939
Epoch 233/1000
 - 28s - loss: 689.7278 - val_loss: 519.5541
Epoch 234/1000
 - 28s - loss: 689.7038 - val_loss: 519.5485
Epoch 235/1000
 - 28s - loss: 689.7012 - val_loss: 519.5488
Epoch 236/1000
 - 28s - loss: 689.7014 - val_loss: 519.5494

Epoch 00236: loss did not improve from 689.69939
Epoch 237/1000
 - 28s - loss: 689.7042 - val_loss: 519.5525
Epoch 238/1000
 - 28s - loss: 689.7241 - val_loss: 519.5861
Epoch 239/1000
 - 27s - loss: 689.7297 - val_loss: 519.5777
Epoch 240/1000
 - 27s - loss: 689.7067 - val_loss: 519.5568

Epoch 00240: loss did not improve from 689.69939
Epoch 241/1000
 - 28s - loss: 689.7252 - val_loss: 519.5597
Epoch 242/1000
 - 28s - loss: 689.7125 - val_loss: 519.5526
Epoch 243/1000
 - 28s - loss: 689.7011 - val_loss: 519.5472
Epoch 244/1000
 - 27s - loss: 689.6998 - val_loss: 519.5462

Epoch 00244: loss did not improve from 689.69939
Epoch 245/1000
 - 27s - loss: 689.7003 - val_loss: 519.5484
Epoch 246/1000
 - 28s - loss: 689.7386 - val_loss: 519.6833
Epoch 247/1000
 - 27s - loss: 689.8880 - val_loss: 519.6951
Epoch 248/1000
 - 27s - loss: 689.8570 - val_loss: 519.7059

Epoch 00248: loss did not improve from 689.69939
Epoch 249/1000
 - 27s - loss: 689.8431 - val_loss: 519.6766
Epoch 250/1000
 - 27s - loss: 689.8393 - val_loss: 519.6752
Epoch 251/1000
 - 27s - loss: 689.8382 - val_loss: 519.6741
Epoch 252/1000
 - 27s - loss: 689.8467 - val_loss: 519.6817

Epoch 00252: loss did not improve from 689.69939
Epoch 253/1000
 - 27s - loss: 689.8418 - val_loss: 519.6795
Epoch 254/1000
 - 27s - loss: 689.8381 - val_loss: 519.6793
Epoch 255/1000
 - 27s - loss: 689.8402 - val_loss: 519.6766
Epoch 256/1000
 - 27s - loss: 689.8396 - val_loss: 519.6884

Epoch 00256: loss did not improve from 689.69939
Epoch 257/1000
 - 28s - loss: 689.8178 - val_loss: 519.6916
Epoch 258/1000
 - 28s - loss: 689.8975 - val_loss: 519.8187
Epoch 259/1000
 - 28s - loss: 689.8977 - val_loss: 519.7280
Epoch 260/1000
 - 28s - loss: 690.4265 - val_loss: 519.7243

Epoch 00260: loss did not improve from 689.69939
Epoch 261/1000
 - 28s - loss: 689.9352 - val_loss: 519.7154
Epoch 262/1000
 - 28s - loss: 689.8740 - val_loss: 519.6986
Epoch 263/1000
 - 28s - loss: 689.8680 - val_loss: 519.6829
Epoch 264/1000
 - 28s - loss: 689.8545 - val_loss: 519.6981

Epoch 00264: loss did not improve from 689.69939
Epoch 265/1000
 - 28s - loss: 689.8615 - val_loss: 519.6866
Epoch 266/1000
 - 28s - loss: 689.8451 - val_loss: 519.6785
Epoch 267/1000
 - 28s - loss: 689.8424 - val_loss: 519.6828
Epoch 268/1000
 - 27s - loss: 689.8356 - val_loss: 519.6890

Epoch 00268: loss did not improve from 689.69939
Epoch 269/1000
 - 28s - loss: 689.8424 - val_loss: 519.6850
Epoch 270/1000
 - 27s - loss: 689.8423 - val_loss: 519.6803
Epoch 271/1000
 - 28s - loss: 689.7413 - val_loss: 519.5475
Epoch 272/1000
 - 28s - loss: 689.6999 - val_loss: 519.5820

Epoch 00272: loss did not improve from 689.69939
Epoch 273/1000
 - 28s - loss: 689.7144 - val_loss: 519.5473
Epoch 274/1000
 - 28s - loss: 689.7010 - val_loss: 519.5489
Epoch 275/1000
 - 28s - loss: 689.6957 - val_loss: 519.5540
Epoch 276/1000
 - 28s - loss: 689.7111 - val_loss: 519.5477

Epoch 00276: loss did not improve from 689.69939
Epoch 277/1000
 - 28s - loss: 689.7462 - val_loss: 519.5798
Epoch 278/1000
 - 28s - loss: 689.7202 - val_loss: 519.7312
Epoch 279/1000
 - 28s - loss: 689.7406 - val_loss: 519.5557
Epoch 280/1000
 - 28s - loss: 689.7004 - val_loss: 519.5546

Epoch 00280: loss did not improve from 689.69939
Epoch 281/1000
 - 28s - loss: 689.6988 - val_loss: 519.5501
Epoch 282/1000
 - 28s - loss: 689.6979 - val_loss: 519.5574
Epoch 283/1000
 - 28s - loss: 689.7036 - val_loss: 519.5485
Epoch 284/1000
 - 28s - loss: 689.6966 - val_loss: 519.5473

Epoch 00284: loss improved from 689.69939 to 689.69660, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 285/1000
 - 28s - loss: 689.6982 - val_loss: 519.6449
Epoch 286/1000
 - 28s - loss: 689.7046 - val_loss: 519.5526
Epoch 287/1000
 - 28s - loss: 689.7045 - val_loss: 519.5471
Epoch 288/1000
 - 28s - loss: 689.6957 - val_loss: 519.5484

Epoch 00288: loss improved from 689.69660 to 689.69566, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 289/1000
 - 28s - loss: 689.6948 - val_loss: 519.5616
Epoch 290/1000
 - 28s - loss: 689.7254 - val_loss: 519.5613
Epoch 291/1000
 - 28s - loss: 689.7200 - val_loss: 519.5608
Epoch 292/1000
 - 28s - loss: 689.6961 - val_loss: 519.5511

Epoch 00292: loss did not improve from 689.69566
Epoch 293/1000
 - 28s - loss: 689.6875 - val_loss: 519.6892
Epoch 294/1000
 - 28s - loss: 692.9992 - val_loss: 519.6642
Epoch 295/1000
 - 28s - loss: 689.7279 - val_loss: 519.5536
Epoch 296/1000
 - 28s - loss: 689.6902 - val_loss: 519.5498

Epoch 00296: loss improved from 689.69566 to 689.69017, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 297/1000
 - 28s - loss: 689.6961 - val_loss: 519.5730
Epoch 298/1000
 - 28s - loss: 689.7016 - val_loss: 519.5595
Epoch 299/1000
 - 28s - loss: 689.6948 - val_loss: 519.5509
Epoch 300/1000
 - 28s - loss: 689.6985 - val_loss: 519.5924

Epoch 00300: loss did not improve from 689.69017
Epoch 301/1000
 - 28s - loss: 689.2445 - val_loss: 519.5491
Epoch 302/1000
 - 28s - loss: 689.6969 - val_loss: 519.5623
Epoch 303/1000
 - 28s - loss: 689.7068 - val_loss: 519.5653
Epoch 304/1000
 - 28s - loss: 689.6956 - val_loss: 519.5493

Epoch 00304: loss did not improve from 689.69017
Epoch 305/1000
 - 28s - loss: 689.6909 - val_loss: 519.5498
Epoch 306/1000
 - 28s - loss: 689.6883 - val_loss: 519.5460
Epoch 307/1000
 - 28s - loss: 689.8117 - val_loss: 519.5739
Epoch 308/1000
 - 28s - loss: 689.6974 - val_loss: 519.5486

Epoch 00308: loss did not improve from 689.69017
Epoch 309/1000
 - 28s - loss: 689.6970 - val_loss: 519.7362
Epoch 310/1000
 - 28s - loss: 689.7129 - val_loss: 519.5618
Epoch 311/1000
 - 28s - loss: 689.6934 - val_loss: 519.5487
Epoch 312/1000
 - 28s - loss: 689.6891 - val_loss: 519.5487

Epoch 00312: loss improved from 689.69017 to 689.68912, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 313/1000
 - 28s - loss: 689.6905 - val_loss: 519.5500
Epoch 314/1000
 - 28s - loss: 689.6912 - val_loss: 519.5489
Epoch 315/1000
 - 28s - loss: 689.6953 - val_loss: 519.5536
Epoch 316/1000
 - 28s - loss: 689.7159 - val_loss: 519.5534

Epoch 00316: loss did not improve from 689.68912
Epoch 317/1000
 - 28s - loss: 689.6908 - val_loss: 519.5476
Epoch 318/1000
 - 28s - loss: 689.6909 - val_loss: 519.5484
Epoch 319/1000
 - 28s - loss: 689.6922 - val_loss: 519.5484
Epoch 320/1000
 - 28s - loss: 689.6899 - val_loss: 519.5493

Epoch 00320: loss did not improve from 689.68912
Epoch 321/1000
 - 28s - loss: 689.7430 - val_loss: 519.7743
Epoch 322/1000
 - 28s - loss: 689.7613 - val_loss: 519.5611
Epoch 323/1000
 - 28s - loss: 689.7031 - val_loss: 519.5534
Epoch 324/1000
 - 28s - loss: 689.6919 - val_loss: 519.5490

Epoch 00324: loss did not improve from 689.68912
Epoch 325/1000
 - 28s - loss: 690.4013 - val_loss: 519.6261
Epoch 326/1000
 - 28s - loss: 689.7863 - val_loss: 519.5758
Epoch 327/1000
 - 28s - loss: 689.7134 - val_loss: 519.5650
Epoch 328/1000
 - 28s - loss: 689.7048 - val_loss: 519.5555

Epoch 00328: loss did not improve from 689.68912
Epoch 329/1000
 - 28s - loss: 689.7067 - val_loss: 519.5700
Epoch 330/1000
 - 27s - loss: 689.6993 - val_loss: 519.5559
Epoch 331/1000
 - 27s - loss: 689.6984 - val_loss: 519.5630
Epoch 332/1000
 - 27s - loss: 689.6960 - val_loss: 519.5514

Epoch 00332: loss did not improve from 689.68912
Epoch 333/1000
 - 27s - loss: 689.6942 - val_loss: 519.5578
Epoch 334/1000
 - 27s - loss: 689.6906 - val_loss: 519.5535
Epoch 335/1000
 - 27s - loss: 689.6958 - val_loss: 519.5537
Epoch 336/1000
 - 27s - loss: 689.8732 - val_loss: 519.5739

Epoch 00336: loss did not improve from 689.68912
Epoch 337/1000
 - 27s - loss: 689.7282 - val_loss: 519.5733
Epoch 338/1000
 - 27s - loss: 689.7121 - val_loss: 519.5693
Epoch 339/1000
 - 28s - loss: 689.7064 - val_loss: 519.5676
Epoch 340/1000
 - 28s - loss: 689.7152 - val_loss: 519.5768

Epoch 00340: loss did not improve from 689.68912
Epoch 341/1000
 - 28s - loss: 689.7116 - val_loss: 519.5599
Epoch 342/1000
 - 28s - loss: 689.7112 - val_loss: 519.5731
Epoch 343/1000
 - 28s - loss: 689.7054 - val_loss: 519.5623
Epoch 344/1000
 - 28s - loss: 689.6999 - val_loss: 519.5640

Epoch 00344: loss did not improve from 689.68912
Epoch 345/1000
 - 28s - loss: 689.7004 - val_loss: 519.5556
Epoch 346/1000
 - 28s - loss: 689.6972 - val_loss: 519.5604
Epoch 347/1000
 - 28s - loss: 689.6980 - val_loss: 519.5872
Epoch 348/1000
 - 28s - loss: 689.7022 - val_loss: 519.5632

Epoch 00348: loss did not improve from 689.68912
Epoch 349/1000
 - 28s - loss: 689.7111 - val_loss: 519.6096
Epoch 350/1000
 - 28s - loss: 689.6974 - val_loss: 519.5572
Epoch 351/1000
 - 28s - loss: 689.6949 - val_loss: 519.5697
Epoch 352/1000
 - 28s - loss: 689.7034 - val_loss: 519.5523

Epoch 00352: loss did not improve from 689.68912
Epoch 353/1000
 - 28s - loss: 689.6953 - val_loss: 519.5523
Epoch 354/1000
 - 28s - loss: 689.6953 - val_loss: 519.5579
Epoch 355/1000
 - 28s - loss: 689.6908 - val_loss: 519.5496
Epoch 356/1000
 - 28s - loss: 689.7044 - val_loss: 519.5612

Epoch 00356: loss did not improve from 689.68912
Epoch 357/1000
 - 28s - loss: 689.7096 - val_loss: 519.6852
Epoch 358/1000
 - 28s - loss: 689.7084 - val_loss: 519.5540
Epoch 359/1000
 - 28s - loss: 689.6919 - val_loss: 519.5488
Epoch 360/1000
 - 28s - loss: 689.6900 - val_loss: 519.5485

Epoch 00360: loss did not improve from 689.68912
Epoch 361/1000
 - 28s - loss: 689.6891 - val_loss: 519.5494
Epoch 362/1000
 - 28s - loss: 689.6916 - val_loss: 519.5496
Epoch 363/1000
 - 28s - loss: 689.6937 - val_loss: 519.5615
Epoch 364/1000
 - 28s - loss: 689.6928 - val_loss: 519.5490

Epoch 00364: loss did not improve from 689.68912
Epoch 365/1000
 - 28s - loss: 689.6923 - val_loss: 519.5502
Epoch 366/1000
 - 28s - loss: 689.7112 - val_loss: 519.6463
Epoch 367/1000
 - 28s - loss: 689.8077 - val_loss: 519.5636
Epoch 368/1000
 - 28s - loss: 689.7302 - val_loss: 519.5709

Epoch 00368: loss did not improve from 689.68912
Epoch 369/1000
 - 28s - loss: 689.7020 - val_loss: 519.5570
Epoch 370/1000
 - 28s - loss: 689.6953 - val_loss: 519.5499
Epoch 371/1000
 - 28s - loss: 689.6947 - val_loss: 519.5660
Epoch 372/1000
 - 28s - loss: 689.6966 - val_loss: 519.5494

Epoch 00372: loss did not improve from 689.68912
Epoch 373/1000
 - 28s - loss: 689.6911 - val_loss: 519.5480
Epoch 374/1000
 - 28s - loss: 689.6890 - val_loss: 519.5481
Epoch 375/1000
 - 28s - loss: 689.6912 - val_loss: 519.5647
Epoch 376/1000
 - 28s - loss: 689.6922 - val_loss: 519.5494

Epoch 00376: loss did not improve from 689.68912
Epoch 377/1000
 - 27s - loss: 689.6906 - val_loss: 519.5529
Epoch 378/1000
 - 28s - loss: 689.6907 - val_loss: 519.5516
Epoch 379/1000
 - 27s - loss: 689.6916 - val_loss: 519.5477
Epoch 380/1000
 - 28s - loss: 689.7146 - val_loss: 519.5989

Epoch 00380: loss did not improve from 689.68912
Epoch 381/1000
 - 27s - loss: 689.7344 - val_loss: 519.5841
Epoch 382/1000
 - 28s - loss: 691.8612 - val_loss: 519.5588
Epoch 383/1000
 - 28s - loss: 689.6890 - val_loss: 519.5492
Epoch 384/1000
 - 28s - loss: 689.6918 - val_loss: 519.5611

Epoch 00384: loss did not improve from 689.68912
Epoch 385/1000
 - 28s - loss: 689.7044 - val_loss: 519.5569
Epoch 386/1000
 - 28s - loss: 691.6951 - val_loss: 519.6997
Epoch 387/1000
 - 28s - loss: 691.4072 - val_loss: 519.6150
Epoch 388/1000
 - 27s - loss: 692.3101 - val_loss: 519.6954

Epoch 00388: loss did not improve from 689.68912
Epoch 389/1000
 - 28s - loss: 690.8834 - val_loss: 519.7148
Epoch 390/1000
 - 28s - loss: 691.3010 - val_loss: 519.7410
Epoch 391/1000
 - 28s - loss: 690.8354 - val_loss: 519.6831
Epoch 392/1000
 - 28s - loss: 690.7337 - val_loss: 519.6767

Epoch 00392: loss did not improve from 689.68912
Epoch 393/1000
 - 28s - loss: 690.7878 - val_loss: 519.6978
Epoch 394/1000
 - 27s - loss: 690.0835 - val_loss: 519.6914
Epoch 395/1000
 - 28s - loss: 690.0702 - val_loss: 519.6902
Epoch 396/1000
 - 27s - loss: 690.0645 - val_loss: 519.6844

Epoch 00396: loss did not improve from 689.68912
Epoch 397/1000
 - 28s - loss: 690.0579 - val_loss: 519.6762
Epoch 398/1000
 - 28s - loss: 690.0437 - val_loss: 519.6815
Epoch 399/1000
 - 27s - loss: 690.0419 - val_loss: 519.6715
Epoch 400/1000
 - 27s - loss: 690.0517 - val_loss: 519.8375

Epoch 00400: loss did not improve from 689.68912
Epoch 401/1000
 - 28s - loss: 690.0609 - val_loss: 519.6744
Epoch 402/1000
 - 28s - loss: 690.0424 - val_loss: 519.6708
Epoch 403/1000
 - 28s - loss: 690.0414 - val_loss: 519.6739
Epoch 404/1000
 - 27s - loss: 690.0962 - val_loss: 519.7078

Epoch 00404: loss did not improve from 689.68912
Epoch 405/1000
 - 28s - loss: 690.0792 - val_loss: 519.7020
Epoch 406/1000
 - 28s - loss: 690.0622 - val_loss: 519.6817
Epoch 407/1000
 - 28s - loss: 690.0513 - val_loss: 519.6803
Epoch 408/1000
 - 28s - loss: 690.0666 - val_loss: 519.6812

Epoch 00408: loss did not improve from 689.68912
Epoch 409/1000
 - 28s - loss: 690.0450 - val_loss: 519.6786
Epoch 410/1000
 - 28s - loss: 690.0400 - val_loss: 519.6767
Epoch 411/1000
 - 28s - loss: 690.0384 - val_loss: 519.6752
Epoch 412/1000
 - 28s - loss: 690.0374 - val_loss: 519.6869

Epoch 00412: loss did not improve from 689.68912
Epoch 413/1000
 - 27s - loss: 690.0406 - val_loss: 519.6808
Epoch 414/1000
 - 27s - loss: 690.0371 - val_loss: 519.6782
Epoch 415/1000
 - 27s - loss: 690.0584 - val_loss: 519.6746
Epoch 416/1000
 - 27s - loss: 691.4612 - val_loss: 519.6826

Epoch 00416: loss did not improve from 689.68912
Epoch 417/1000
 - 28s - loss: 693.9094 - val_loss: 526.4070
Epoch 418/1000
 - 28s - loss: 698.1018 - val_loss: 526.3865
Epoch 419/1000
 - 28s - loss: 697.8599 - val_loss: 526.3969
Epoch 420/1000
 - 28s - loss: 698.0179 - val_loss: 526.3727

Epoch 00420: loss did not improve from 689.68912
Epoch 421/1000
 - 28s - loss: 697.9140 - val_loss: 526.3733
Epoch 422/1000
 - 28s - loss: 697.9051 - val_loss: 526.3731
Epoch 423/1000
 - 27s - loss: 697.9036 - val_loss: 526.3719
Epoch 424/1000
 - 27s - loss: 697.9031 - val_loss: 526.3716

Epoch 00424: loss did not improve from 689.68912
Epoch 425/1000
 - 27s - loss: 697.9028 - val_loss: 526.3714
Epoch 426/1000
 - 27s - loss: 697.9026 - val_loss: 526.3712
Epoch 427/1000
 - 27s - loss: 695.8089 - val_loss: 520.6062
Epoch 428/1000
 - 27s - loss: 690.9795 - val_loss: 519.9762

Epoch 00428: loss did not improve from 689.68912
Epoch 429/1000
 - 27s - loss: 691.3993 - val_loss: 520.0452
Epoch 430/1000
 - 27s - loss: 690.5547 - val_loss: 520.1553
Epoch 431/1000
 - 27s - loss: 690.6511 - val_loss: 520.1073
Epoch 432/1000
 - 27s - loss: 690.6257 - val_loss: 520.1047

Epoch 00432: loss did not improve from 689.68912
Epoch 433/1000
 - 28s - loss: 690.6186 - val_loss: 520.1069
Epoch 434/1000
 - 27s - loss: 690.6219 - val_loss: 520.0958
Epoch 435/1000
 - 27s - loss: 690.6140 - val_loss: 520.1951
Epoch 436/1000
 - 27s - loss: 690.6765 - val_loss: 520.2137

Epoch 00436: loss did not improve from 689.68912
Epoch 437/1000
 - 27s - loss: 690.6276 - val_loss: 520.0900
Epoch 438/1000
 - 27s - loss: 690.6095 - val_loss: 520.0869
Epoch 439/1000
 - 28s - loss: 690.6063 - val_loss: 520.0843
Epoch 440/1000
 - 28s - loss: 690.6035 - val_loss: 520.0827

Epoch 00440: loss did not improve from 689.68912
Epoch 441/1000
 - 28s - loss: 690.6014 - val_loss: 520.0807
Epoch 442/1000
 - 28s - loss: 690.5996 - val_loss: 520.0788
Epoch 443/1000
 - 28s - loss: 690.5984 - val_loss: 520.0799
Epoch 444/1000
 - 28s - loss: 690.5974 - val_loss: 520.0769

Epoch 00444: loss did not improve from 689.68912
Epoch 445/1000
 - 27s - loss: 690.5957 - val_loss: 520.0768
Epoch 446/1000
 - 28s - loss: 690.5956 - val_loss: 520.0803
Epoch 447/1000
 - 27s - loss: 690.5951 - val_loss: 520.0754
Epoch 448/1000
 - 27s - loss: 691.3405 - val_loss: 520.3148

Epoch 00448: loss did not improve from 689.68912
Epoch 449/1000
 - 28s - loss: 690.7764 - val_loss: 520.1275
Epoch 450/1000
 - 28s - loss: 690.6247 - val_loss: 520.1132
Epoch 451/1000
 - 28s - loss: 690.6261 - val_loss: 520.1023
Epoch 452/1000
 - 28s - loss: 690.6241 - val_loss: 520.1041

Epoch 00452: loss did not improve from 689.68912
Epoch 453/1000
 - 28s - loss: 690.6208 - val_loss: 520.1110
Epoch 454/1000
 - 28s - loss: 690.6023 - val_loss: 520.0922
Epoch 455/1000
 - 28s - loss: 690.6038 - val_loss: 520.0986
Epoch 456/1000
 - 27s - loss: 690.6005 - val_loss: 520.0926

Epoch 00456: loss did not improve from 689.68912
Epoch 457/1000
 - 27s - loss: 690.5934 - val_loss: 519.9880
Epoch 458/1000
 - 27s - loss: 690.3855 - val_loss: 519.9530
Epoch 459/1000
 - 27s - loss: 690.3564 - val_loss: 519.9464
Epoch 460/1000
 - 27s - loss: 690.3456 - val_loss: 519.9397

Epoch 00460: loss did not improve from 689.68912
Epoch 461/1000
 - 27s - loss: 690.3423 - val_loss: 519.9382
Epoch 462/1000
 - 28s - loss: 690.3367 - val_loss: 519.9366
Epoch 463/1000
 - 28s - loss: 690.3442 - val_loss: 519.9406
Epoch 464/1000
 - 28s - loss: 690.3318 - val_loss: 519.9358

Epoch 00464: loss did not improve from 689.68912
Epoch 465/1000
 - 28s - loss: 690.3221 - val_loss: 519.9336
Epoch 466/1000
 - 28s - loss: 690.3212 - val_loss: 519.9330
Epoch 467/1000
 - 28s - loss: 690.3257 - val_loss: 519.9949
Epoch 468/1000
 - 28s - loss: 692.0738 - val_loss: 520.4680

Epoch 00468: loss did not improve from 689.68912
Epoch 469/1000
 - 28s - loss: 690.8722 - val_loss: 520.3608
Epoch 470/1000
 - 28s - loss: 690.8757 - val_loss: 520.3549
Epoch 471/1000
 - 28s - loss: 690.8184 - val_loss: 520.3564
Epoch 472/1000
 - 28s - loss: 690.7892 - val_loss: 520.3353

Epoch 00472: loss did not improve from 689.68912
Epoch 473/1000
 - 28s - loss: 690.7744 - val_loss: 520.3264
Epoch 474/1000
 - 28s - loss: 690.6975 - val_loss: 520.2084
Epoch 475/1000
 - 28s - loss: 690.6252 - val_loss: 520.1996
Epoch 476/1000
 - 28s - loss: 690.6269 - val_loss: 520.1983

Epoch 00476: loss did not improve from 689.68912
Epoch 477/1000
 - 28s - loss: 690.6209 - val_loss: 520.1946
Epoch 478/1000
 - 28s - loss: 690.6286 - val_loss: 520.1977
Epoch 479/1000
 - 28s - loss: 690.6216 - val_loss: 520.1984
Epoch 480/1000
 - 28s - loss: 690.6221 - val_loss: 520.2026

Epoch 00480: loss did not improve from 689.68912
Epoch 481/1000
 - 28s - loss: 690.6186 - val_loss: 520.2160
Epoch 482/1000
 - 28s - loss: 690.6222 - val_loss: 520.1964
Epoch 483/1000
 - 28s - loss: 690.6057 - val_loss: 520.1864
Epoch 484/1000
 - 28s - loss: 690.6063 - val_loss: 520.1871

Epoch 00484: loss did not improve from 689.68912
Epoch 485/1000
 - 28s - loss: 690.6002 - val_loss: 520.1837
Epoch 486/1000
 - 28s - loss: 690.6001 - val_loss: 520.1947
Epoch 487/1000
 - 28s - loss: 690.5962 - val_loss: 520.1897
Epoch 488/1000
 - 28s - loss: 690.6011 - val_loss: 520.1963

Epoch 00488: loss did not improve from 689.68912
Epoch 489/1000
 - 28s - loss: 690.6030 - val_loss: 520.1850
Epoch 490/1000
 - 27s - loss: 690.3271 - val_loss: 520.1919
Epoch 491/1000
 - 28s - loss: 691.9955 - val_loss: 526.4053
Epoch 492/1000
 - 27s - loss: 697.9671 - val_loss: 526.4038

Epoch 00492: loss did not improve from 689.68912
Epoch 493/1000
 - 28s - loss: 697.9587 - val_loss: 526.4016
Epoch 494/1000
 - 27s - loss: 697.9562 - val_loss: 526.3980
Epoch 495/1000
 - 27s - loss: 697.9483 - val_loss: 526.3950
Epoch 496/1000
 - 27s - loss: 697.9551 - val_loss: 526.3936

Epoch 00496: loss did not improve from 689.68912
Epoch 497/1000
 - 28s - loss: 697.9540 - val_loss: 526.3926
Epoch 498/1000
 - 28s - loss: 697.9530 - val_loss: 526.3919
Epoch 499/1000
 - 28s - loss: 697.9522 - val_loss: 526.3913
Epoch 500/1000
 - 27s - loss: 697.9515 - val_loss: 526.3904

Epoch 00500: loss did not improve from 689.68912
Epoch 501/1000
 - 27s - loss: 697.9509 - val_loss: 526.3900
Epoch 502/1000
 - 28s - loss: 697.9514 - val_loss: 526.3898
Epoch 503/1000
 - 28s - loss: 697.9504 - val_loss: 526.3895
Epoch 504/1000
 - 28s - loss: 697.9502 - val_loss: 526.3894

Epoch 00504: loss did not improve from 689.68912
Epoch 505/1000
 - 28s - loss: 697.9500 - val_loss: 526.3892
Epoch 506/1000
 - 28s - loss: 695.0456 - val_loss: 520.5891
Epoch 507/1000
 - 28s - loss: 690.9821 - val_loss: 520.5405
Epoch 508/1000
 - 28s - loss: 690.8636 - val_loss: 520.3532

Epoch 00508: loss did not improve from 689.68912
Epoch 509/1000
 - 28s - loss: 690.7636 - val_loss: 520.3499
Epoch 510/1000
 - 28s - loss: 692.1672 - val_loss: 520.3491
Epoch 511/1000
 - 28s - loss: 690.7545 - val_loss: 520.3367
Epoch 512/1000
 - 28s - loss: 690.7482 - val_loss: 520.3349

Epoch 00512: loss did not improve from 689.68912
Epoch 513/1000
 - 28s - loss: 690.7457 - val_loss: 520.3339
Epoch 514/1000
 - 28s - loss: 690.7437 - val_loss: 520.3331
Epoch 515/1000
 - 27s - loss: 690.7437 - val_loss: 520.3345
Epoch 516/1000
 - 28s - loss: 690.7370 - val_loss: 520.3341

Epoch 00516: loss did not improve from 689.68912
Epoch 517/1000
 - 28s - loss: 690.7380 - val_loss: 520.3326
Epoch 518/1000
 - 28s - loss: 690.7362 - val_loss: 520.3302
Epoch 519/1000
 - 28s - loss: 690.7370 - val_loss: 520.3355
Epoch 520/1000
 - 28s - loss: 690.6333 - val_loss: 520.3419

Epoch 00520: loss did not improve from 689.68912
Epoch 521/1000
 - 27s - loss: 690.7359 - val_loss: 520.3253
Epoch 522/1000
 - 27s - loss: 690.7215 - val_loss: 520.3253
Epoch 523/1000
 - 27s - loss: 690.7246 - val_loss: 520.3253
Epoch 524/1000
 - 28s - loss: 692.1042 - val_loss: 520.3362

Epoch 00524: loss did not improve from 689.68912
Epoch 525/1000
 - 28s - loss: 692.0096 - val_loss: 520.3287
Epoch 526/1000
 - 28s - loss: 690.7337 - val_loss: 520.3258
Epoch 527/1000
 - 28s - loss: 690.7289 - val_loss: 520.3244
Epoch 528/1000
 - 28s - loss: 690.7272 - val_loss: 520.3247

Epoch 00528: loss did not improve from 689.68912
Epoch 529/1000
 - 28s - loss: 690.7254 - val_loss: 520.3283
Epoch 530/1000
 - 27s - loss: 690.7244 - val_loss: 520.3245
Epoch 531/1000
 - 27s - loss: 690.7236 - val_loss: 520.3243
Epoch 532/1000
 - 27s - loss: 690.7420 - val_loss: 520.3237

Epoch 00532: loss did not improve from 689.68912
Epoch 533/1000
 - 27s - loss: 690.7253 - val_loss: 520.3233
Epoch 534/1000
 - 28s - loss: 690.7241 - val_loss: 520.3222
Epoch 535/1000
 - 28s - loss: 690.7231 - val_loss: 520.3229
Epoch 536/1000
 - 28s - loss: 690.7222 - val_loss: 520.3219

Epoch 00536: loss did not improve from 689.68912
Epoch 537/1000
 - 28s - loss: 690.7210 - val_loss: 520.3328
Epoch 538/1000
 - 28s - loss: 690.7297 - val_loss: 520.3243
Epoch 539/1000
 - 28s - loss: 690.7623 - val_loss: 520.4816
Epoch 540/1000
 - 28s - loss: 692.3296 - val_loss: 520.3308

Epoch 00540: loss did not improve from 689.68912
Epoch 541/1000
 - 28s - loss: 692.0370 - val_loss: 520.3235
Epoch 542/1000
 - 27s - loss: 691.9848 - val_loss: 520.3260
Epoch 543/1000
 - 27s - loss: 692.3076 - val_loss: 520.3241
Epoch 544/1000
 - 28s - loss: 692.3120 - val_loss: 520.3216

Epoch 00544: loss did not improve from 689.68912
Epoch 545/1000
 - 28s - loss: 692.3015 - val_loss: 520.3216
Epoch 546/1000
 - 28s - loss: 692.4298 - val_loss: 523.5344
Epoch 547/1000
 - 27s - loss: 691.3469 - val_loss: 520.5789
Epoch 548/1000
 - 27s - loss: 690.9588 - val_loss: 520.5598

Epoch 00548: loss did not improve from 689.68912
Epoch 549/1000
 - 27s - loss: 690.9331 - val_loss: 520.5183
Epoch 550/1000
 - 27s - loss: 690.8788 - val_loss: 520.4149
Epoch 551/1000
 - 27s - loss: 690.7909 - val_loss: 520.3839
Epoch 552/1000
 - 27s - loss: 690.7660 - val_loss: 520.4086

Epoch 00552: loss did not improve from 689.68912
Epoch 553/1000
 - 27s - loss: 690.7630 - val_loss: 520.3692
Epoch 554/1000
 - 27s - loss: 690.7703 - val_loss: 520.3995
Epoch 555/1000
 - 28s - loss: 690.7732 - val_loss: 520.3420
Epoch 556/1000
 - 27s - loss: 690.7479 - val_loss: 520.3371

Epoch 00556: loss did not improve from 689.68912
Epoch 557/1000
 - 27s - loss: 690.7419 - val_loss: 520.3331
Epoch 558/1000
 - 27s - loss: 690.7377 - val_loss: 520.3312
Epoch 559/1000
 - 27s - loss: 690.7337 - val_loss: 520.3283
Epoch 560/1000
 - 27s - loss: 690.7340 - val_loss: 520.3303

Epoch 00560: loss did not improve from 689.68912
Epoch 561/1000
 - 28s - loss: 690.7322 - val_loss: 520.3267
Epoch 562/1000
 - 27s - loss: 690.7294 - val_loss: 520.3264
Epoch 563/1000
 - 28s - loss: 690.7318 - val_loss: 520.3352
Epoch 564/1000
 - 28s - loss: 690.7412 - val_loss: 520.3317

Epoch 00564: loss did not improve from 689.68912
Epoch 565/1000
 - 28s - loss: 690.7277 - val_loss: 520.3544
Epoch 566/1000
 - 27s - loss: 690.7462 - val_loss: 520.3286
Epoch 567/1000
 - 27s - loss: 690.5163 - val_loss: 520.3360
Epoch 568/1000
 - 27s - loss: 691.6003 - val_loss: 520.3609

Epoch 00568: loss did not improve from 689.68912
Epoch 569/1000
 - 27s - loss: 690.7398 - val_loss: 520.3296
Epoch 570/1000
 - 28s - loss: 690.7369 - val_loss: 520.3632
Epoch 571/1000
 - 28s - loss: 690.6316 - val_loss: 520.1799
Epoch 572/1000
 - 28s - loss: 690.5730 - val_loss: 520.1825

Epoch 00572: loss did not improve from 689.68912
Epoch 573/1000
 - 28s - loss: 690.5729 - val_loss: 520.1933
Epoch 574/1000
 - 28s - loss: 690.5822 - val_loss: 520.1819
Epoch 575/1000
 - 27s - loss: 690.5710 - val_loss: 520.1778
Epoch 576/1000
 - 27s - loss: 690.5684 - val_loss: 520.1764

Epoch 00576: loss did not improve from 689.68912
Epoch 577/1000
 - 27s - loss: 690.6000 - val_loss: 520.1820
Epoch 578/1000
 - 27s - loss: 690.5767 - val_loss: 520.1781
Epoch 579/1000
 - 27s - loss: 690.5697 - val_loss: 520.1768
Epoch 580/1000
 - 27s - loss: 690.3979 - val_loss: 520.0899

Epoch 00580: loss did not improve from 689.68912
Epoch 581/1000
 - 27s - loss: 690.2905 - val_loss: 520.0895
Epoch 582/1000
 - 28s - loss: 690.2910 - val_loss: 520.1316
Epoch 583/1000
 - 28s - loss: 690.2889 - val_loss: 520.0876
Epoch 584/1000
 - 28s - loss: 690.2843 - val_loss: 520.0982

Epoch 00584: loss did not improve from 689.68912
Epoch 585/1000
 - 27s - loss: 690.2825 - val_loss: 520.0827
Epoch 586/1000
 - 28s - loss: 690.2823 - val_loss: 520.0832
Epoch 587/1000
 - 28s - loss: 690.2803 - val_loss: 520.0838
Epoch 588/1000
 - 27s - loss: 690.2797 - val_loss: 520.0903

Epoch 00588: loss did not improve from 689.68912
Epoch 589/1000
 - 27s - loss: 690.3775 - val_loss: 520.1170
Epoch 590/1000
 - 27s - loss: 690.3083 - val_loss: 520.0905
Epoch 591/1000
 - 27s - loss: 690.2960 - val_loss: 520.0822
Epoch 592/1000
 - 28s - loss: 690.2977 - val_loss: 520.0835

Epoch 00592: loss did not improve from 689.68912
Epoch 593/1000
 - 27s - loss: 690.2915 - val_loss: 520.1015
Epoch 594/1000
 - 27s - loss: 690.2866 - val_loss: 520.1481
Epoch 595/1000
 - 27s - loss: 690.2970 - val_loss: 520.0896
Epoch 596/1000
 - 27s - loss: 690.2875 - val_loss: 520.0841

Epoch 00596: loss did not improve from 689.68912
Epoch 597/1000
 - 27s - loss: 690.2830 - val_loss: 520.0863
Epoch 598/1000
 - 28s - loss: 690.2806 - val_loss: 520.0837
Epoch 599/1000
 - 28s - loss: 690.2835 - val_loss: 520.0894
Epoch 600/1000
 - 27s - loss: 690.2922 - val_loss: 520.0840

Epoch 00600: loss did not improve from 689.68912
Epoch 601/1000
 - 28s - loss: 690.2841 - val_loss: 520.0839
Epoch 602/1000
 - 28s - loss: 690.2918 - val_loss: 520.1138
Epoch 603/1000
 - 27s - loss: 690.2850 - val_loss: 520.0845
Epoch 604/1000
 - 27s - loss: 690.2878 - val_loss: 520.0948

Epoch 00604: loss did not improve from 689.68912
Epoch 605/1000
 - 27s - loss: 690.2815 - val_loss: 520.0818
Epoch 606/1000
 - 27s - loss: 690.2799 - val_loss: 520.0823
Epoch 607/1000
 - 27s - loss: 690.2855 - val_loss: 520.0863
Epoch 608/1000
 - 28s - loss: 690.2831 - val_loss: 520.0824

Epoch 00608: loss did not improve from 689.68912
Epoch 609/1000
 - 28s - loss: 690.2793 - val_loss: 520.0804
Epoch 610/1000
 - 27s - loss: 690.2836 - val_loss: 520.0949
Epoch 611/1000
 - 27s - loss: 690.2898 - val_loss: 520.0839
Epoch 612/1000
 - 28s - loss: 690.2862 - val_loss: 520.0863

Epoch 00612: loss did not improve from 689.68912
Epoch 613/1000
 - 28s - loss: 690.2968 - val_loss: 520.1346
Epoch 614/1000
 - 28s - loss: 690.2883 - val_loss: 520.1198
Epoch 615/1000
 - 28s - loss: 690.3456 - val_loss: 520.0851
Epoch 616/1000
 - 27s - loss: 690.3018 - val_loss: 520.0903

Epoch 00616: loss did not improve from 689.68912
Epoch 617/1000
 - 28s - loss: 690.2815 - val_loss: 520.0827
Epoch 618/1000
 - 28s - loss: 690.2765 - val_loss: 520.0861
Epoch 619/1000
 - 27s - loss: 690.2789 - val_loss: 520.0827
Epoch 620/1000
 - 28s - loss: 690.2822 - val_loss: 520.0854

Epoch 00620: loss did not improve from 689.68912
Epoch 621/1000
 - 27s - loss: 690.2799 - val_loss: 520.0848
Epoch 622/1000
 - 28s - loss: 690.2790 - val_loss: 520.0801
Epoch 623/1000
 - 28s - loss: 690.2717 - val_loss: 520.0793
Epoch 624/1000
 - 28s - loss: 690.1878 - val_loss: 519.9825

Epoch 00624: loss did not improve from 689.68912
Epoch 625/1000
 - 28s - loss: 690.1688 - val_loss: 519.9272
Epoch 626/1000
 - 28s - loss: 690.1372 - val_loss: 519.9292
Epoch 627/1000
 - 28s - loss: 690.1214 - val_loss: 519.9254
Epoch 628/1000
 - 28s - loss: 690.1339 - val_loss: 519.9253

Epoch 00628: loss did not improve from 689.68912
Epoch 629/1000
 - 28s - loss: 690.1019 - val_loss: 519.8076
Epoch 630/1000
 - 28s - loss: 689.9878 - val_loss: 519.8084
Epoch 631/1000
 - 28s - loss: 689.9049 - val_loss: 519.7042
Epoch 632/1000
 - 28s - loss: 689.8829 - val_loss: 519.6979

Epoch 00632: loss did not improve from 689.68912
Epoch 633/1000
 - 28s - loss: 689.8600 - val_loss: 519.6927
Epoch 634/1000
 - 28s - loss: 689.8591 - val_loss: 519.6925
Epoch 635/1000
 - 28s - loss: 689.8585 - val_loss: 519.6914
Epoch 636/1000
 - 28s - loss: 689.8590 - val_loss: 519.6957

Epoch 00636: loss did not improve from 689.68912
Epoch 637/1000
 - 28s - loss: 689.8638 - val_loss: 519.7191
Epoch 638/1000
 - 28s - loss: 689.8637 - val_loss: 519.7235
Epoch 639/1000
 - 28s - loss: 689.8605 - val_loss: 519.6927
Epoch 640/1000
 - 28s - loss: 689.8632 - val_loss: 519.6938

Epoch 00640: loss did not improve from 689.68912
Epoch 641/1000
 - 28s - loss: 689.8714 - val_loss: 519.7047
Epoch 642/1000
 - 28s - loss: 689.8603 - val_loss: 519.6922
Epoch 643/1000
 - 28s - loss: 689.8608 - val_loss: 519.7218
Epoch 644/1000
 - 28s - loss: 689.8751 - val_loss: 519.7113

Epoch 00644: loss did not improve from 689.68912
Epoch 645/1000
 - 28s - loss: 689.8849 - val_loss: 519.7486
Epoch 646/1000
 - 27s - loss: 689.8676 - val_loss: 519.6988
Epoch 647/1000
 - 27s - loss: 689.8605 - val_loss: 519.6936
Epoch 648/1000
 - 27s - loss: 689.8583 - val_loss: 519.6921

Epoch 00648: loss did not improve from 689.68912
Epoch 649/1000
 - 27s - loss: 689.8666 - val_loss: 519.7146
Epoch 650/1000
 - 28s - loss: 689.8715 - val_loss: 519.6945
Epoch 651/1000
 - 28s - loss: 689.8603 - val_loss: 519.6955
Epoch 652/1000
 - 28s - loss: 689.8586 - val_loss: 519.7066

Epoch 00652: loss did not improve from 689.68912
Epoch 653/1000
 - 28s - loss: 689.8613 - val_loss: 519.6909
Epoch 654/1000
 - 28s - loss: 689.8582 - val_loss: 519.6908
Epoch 655/1000
 - 27s - loss: 689.8595 - val_loss: 519.6930
Epoch 656/1000
 - 28s - loss: 689.8596 - val_loss: 519.6910

Epoch 00656: loss did not improve from 689.68912
Epoch 657/1000
 - 28s - loss: 689.8583 - val_loss: 519.6917
Epoch 658/1000
 - 28s - loss: 689.8574 - val_loss: 519.6973
Epoch 659/1000
 - 28s - loss: 689.8650 - val_loss: 519.6997
Epoch 660/1000
 - 28s - loss: 689.8613 - val_loss: 519.6917

Epoch 00660: loss did not improve from 689.68912
Epoch 661/1000
 - 28s - loss: 689.8737 - val_loss: 519.7012
Epoch 662/1000
 - 28s - loss: 689.8644 - val_loss: 519.7054
Epoch 663/1000
 - 28s - loss: 689.8616 - val_loss: 519.6966
Epoch 664/1000
 - 28s - loss: 689.8578 - val_loss: 519.7132

Epoch 00664: loss did not improve from 689.68912
Epoch 665/1000
 - 28s - loss: 689.8702 - val_loss: 519.6924
Epoch 666/1000
 - 28s - loss: 689.8586 - val_loss: 519.6913
Epoch 667/1000
 - 28s - loss: 689.8557 - val_loss: 519.6902
Epoch 668/1000
 - 28s - loss: 689.8591 - val_loss: 519.6944

Epoch 00668: loss did not improve from 689.68912
Epoch 669/1000
 - 28s - loss: 689.8718 - val_loss: 519.7309
Epoch 670/1000
 - 27s - loss: 689.8709 - val_loss: 519.6996
Epoch 671/1000
 - 28s - loss: 689.8591 - val_loss: 519.6930
Epoch 672/1000
 - 28s - loss: 689.8586 - val_loss: 519.7052

Epoch 00672: loss did not improve from 689.68912
Epoch 673/1000
 - 28s - loss: 689.8653 - val_loss: 519.7099
Epoch 674/1000
 - 27s - loss: 689.8851 - val_loss: 519.7110
Epoch 675/1000
 - 28s - loss: 689.8792 - val_loss: 519.6987
Epoch 676/1000
 - 28s - loss: 689.8618 - val_loss: 519.6939

Epoch 00676: loss did not improve from 689.68912
Epoch 677/1000
 - 28s - loss: 689.8574 - val_loss: 519.6926
Epoch 678/1000
 - 28s - loss: 689.8571 - val_loss: 519.6989
Epoch 679/1000
 - 28s - loss: 689.8582 - val_loss: 519.6907
Epoch 680/1000
 - 28s - loss: 689.8608 - val_loss: 519.6953

Epoch 00680: loss did not improve from 689.68912
Epoch 681/1000
 - 28s - loss: 689.8592 - val_loss: 519.6976
Epoch 682/1000
 - 28s - loss: 689.8582 - val_loss: 519.6954
Epoch 683/1000
 - 28s - loss: 689.8653 - val_loss: 519.7434
Epoch 684/1000
 - 28s - loss: 689.8695 - val_loss: 519.6998

Epoch 00684: loss did not improve from 689.68912
Epoch 685/1000
 - 28s - loss: 689.8878 - val_loss: 519.7989
Epoch 686/1000
 - 28s - loss: 689.7190 - val_loss: 519.5561
Epoch 687/1000
 - 28s - loss: 689.6996 - val_loss: 519.5500
Epoch 688/1000
 - 28s - loss: 689.6966 - val_loss: 519.5473

Epoch 00688: loss did not improve from 689.68912
Epoch 689/1000
 - 28s - loss: 689.6948 - val_loss: 519.5488
Epoch 690/1000
 - 28s - loss: 689.7038 - val_loss: 519.5487
Epoch 691/1000
 - 28s - loss: 689.7005 - val_loss: 519.5635
Epoch 692/1000
 - 28s - loss: 689.7004 - val_loss: 519.5531

Epoch 00692: loss did not improve from 689.68912
Epoch 693/1000
 - 28s - loss: 689.6989 - val_loss: 519.5790
Epoch 694/1000
 - 28s - loss: 689.7103 - val_loss: 519.5509
Epoch 695/1000
 - 28s - loss: 689.7040 - val_loss: 519.5506
Epoch 696/1000
 - 28s - loss: 689.7035 - val_loss: 519.5904

Epoch 00696: loss did not improve from 689.68912
Epoch 697/1000
 - 28s - loss: 689.7100 - val_loss: 519.5540
Epoch 698/1000
 - 28s - loss: 689.7014 - val_loss: 519.5495
Epoch 699/1000
 - 28s - loss: 689.6998 - val_loss: 519.5490
Epoch 700/1000
 - 28s - loss: 689.7033 - val_loss: 519.6581

Epoch 00700: loss did not improve from 689.68912
Epoch 701/1000
 - 28s - loss: 689.7126 - val_loss: 519.5506
Epoch 702/1000
 - 28s - loss: 689.7000 - val_loss: 519.5477
Epoch 703/1000
 - 28s - loss: 689.6985 - val_loss: 519.5477
Epoch 704/1000
 - 28s - loss: 689.6981 - val_loss: 519.5474

Epoch 00704: loss did not improve from 689.68912
Epoch 705/1000
 - 28s - loss: 689.6992 - val_loss: 519.5479
Epoch 706/1000
 - 28s - loss: 689.6987 - val_loss: 519.5497
Epoch 707/1000
 - 28s - loss: 689.7023 - val_loss: 519.5471
Epoch 708/1000
 - 28s - loss: 689.7276 - val_loss: 519.5675

Epoch 00708: loss did not improve from 689.68912
Epoch 709/1000
 - 28s - loss: 689.7097 - val_loss: 519.5508
Epoch 710/1000
 - 28s - loss: 689.7009 - val_loss: 519.5793
Epoch 711/1000
 - 28s - loss: 689.6992 - val_loss: 519.5461
Epoch 712/1000
 - 28s - loss: 689.6977 - val_loss: 519.5452

Epoch 00712: loss did not improve from 689.68912
Epoch 713/1000
 - 28s - loss: 689.6976 - val_loss: 519.5455
Epoch 714/1000
 - 28s - loss: 689.6989 - val_loss: 519.5470
Epoch 715/1000
 - 28s - loss: 689.6992 - val_loss: 519.5447
Epoch 716/1000
 - 28s - loss: 689.6991 - val_loss: 519.5460

Epoch 00716: loss did not improve from 689.68912
Epoch 717/1000
 - 28s - loss: 689.6987 - val_loss: 519.5513
Epoch 718/1000
 - 27s - loss: 689.7021 - val_loss: 519.5479
Epoch 719/1000
 - 27s - loss: 689.6989 - val_loss: 519.5511
Epoch 720/1000
 - 27s - loss: 689.6994 - val_loss: 519.5523

Epoch 00720: loss did not improve from 689.68912
Epoch 721/1000
 - 28s - loss: 689.7075 - val_loss: 519.5520
Epoch 722/1000
 - 28s - loss: 689.7086 - val_loss: 519.5495
Epoch 723/1000
 - 28s - loss: 689.6994 - val_loss: 519.5462
Epoch 724/1000
 - 28s - loss: 689.7021 - val_loss: 519.5505

Epoch 00724: loss did not improve from 689.68912
Epoch 725/1000
 - 28s - loss: 689.6992 - val_loss: 519.5456
Epoch 726/1000
 - 28s - loss: 689.7430 - val_loss: 519.6338
Epoch 727/1000
 - 28s - loss: 689.7258 - val_loss: 519.5660
Epoch 728/1000
 - 28s - loss: 689.7041 - val_loss: 519.5498

Epoch 00728: loss did not improve from 689.68912
Epoch 729/1000
 - 28s - loss: 689.6997 - val_loss: 519.5484
Epoch 730/1000
 - 28s - loss: 689.6985 - val_loss: 519.5474
Epoch 731/1000
 - 28s - loss: 689.7000 - val_loss: 519.5542
Epoch 732/1000
 - 28s - loss: 689.6996 - val_loss: 519.5496

Epoch 00732: loss did not improve from 689.68912
Epoch 733/1000
 - 28s - loss: 689.6989 - val_loss: 519.5455
Epoch 734/1000
 - 28s - loss: 689.7180 - val_loss: 519.5481
Epoch 735/1000
 - 28s - loss: 689.7018 - val_loss: 519.5555
Epoch 736/1000
 - 28s - loss: 689.7038 - val_loss: 519.5462

Epoch 00736: loss did not improve from 689.68912
Epoch 737/1000
 - 28s - loss: 689.6989 - val_loss: 519.5478
Epoch 738/1000
 - 28s - loss: 689.6981 - val_loss: 519.5475
Epoch 739/1000
 - 28s - loss: 689.6998 - val_loss: 519.5492
Epoch 740/1000
 - 28s - loss: 689.7049 - val_loss: 519.5497

Epoch 00740: loss did not improve from 689.68912
Epoch 741/1000
 - 28s - loss: 689.6998 - val_loss: 519.5497
Epoch 742/1000
 - 28s - loss: 689.7065 - val_loss: 519.5549
Epoch 743/1000
 - 27s - loss: 689.7020 - val_loss: 519.5493
Epoch 744/1000
 - 28s - loss: 689.7001 - val_loss: 519.5464

Epoch 00744: loss did not improve from 689.68912
Epoch 745/1000
 - 28s - loss: 689.7017 - val_loss: 519.5463
Epoch 746/1000
 - 28s - loss: 689.7086 - val_loss: 519.5678
Epoch 747/1000
 - 28s - loss: 689.7055 - val_loss: 519.5840
Epoch 748/1000
 - 28s - loss: 689.7175 - val_loss: 519.5476

Epoch 00748: loss did not improve from 689.68912
Epoch 749/1000
 - 28s - loss: 689.6996 - val_loss: 519.5551
Epoch 750/1000
 - 28s - loss: 689.7015 - val_loss: 519.5460
Epoch 751/1000
 - 28s - loss: 689.6976 - val_loss: 519.5492
Epoch 752/1000
 - 27s - loss: 689.6986 - val_loss: 519.5586

Epoch 00752: loss did not improve from 689.68912
Epoch 753/1000
 - 27s - loss: 689.7013 - val_loss: 519.5459
Epoch 754/1000
 - 28s - loss: 689.7028 - val_loss: 519.5461
Epoch 755/1000
 - 28s - loss: 689.7010 - val_loss: 519.5481
Epoch 756/1000
 - 28s - loss: 689.7036 - val_loss: 519.5507

Epoch 00756: loss did not improve from 689.68912
Epoch 757/1000
 - 27s - loss: 689.7089 - val_loss: 519.5511
Epoch 758/1000
 - 28s - loss: 689.7054 - val_loss: 519.5474
Epoch 759/1000
 - 28s - loss: 689.7136 - val_loss: 519.6024
Epoch 760/1000
 - 28s - loss: 689.7079 - val_loss: 519.5473

Epoch 00760: loss did not improve from 689.68912
Epoch 761/1000
 - 27s - loss: 689.6993 - val_loss: 519.5512
Epoch 762/1000
 - 27s - loss: 689.6981 - val_loss: 519.5468
Epoch 763/1000
 - 27s - loss: 689.6999 - val_loss: 519.5521
Epoch 764/1000
 - 28s - loss: 689.7245 - val_loss: 519.5611

Epoch 00764: loss did not improve from 689.68912
Epoch 765/1000
 - 28s - loss: 689.8499 - val_loss: 519.4657
Epoch 766/1000
 - 28s - loss: 689.5891 - val_loss: 519.4524
Epoch 767/1000
 - 27s - loss: 689.5970 - val_loss: 519.4497
Epoch 768/1000
 - 27s - loss: 689.5924 - val_loss: 519.4643

Epoch 00768: loss improved from 689.68912 to 689.59238, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 769/1000
 - 27s - loss: 689.5834 - val_loss: 519.4464
Epoch 770/1000
 - 28s - loss: 689.5917 - val_loss: 519.4671
Epoch 771/1000
 - 28s - loss: 689.5869 - val_loss: 519.4637
Epoch 772/1000
 - 28s - loss: 689.5743 - val_loss: 519.4479

Epoch 00772: loss improved from 689.59238 to 689.57433, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 773/1000
 - 28s - loss: 689.5814 - val_loss: 519.4409
Epoch 774/1000
 - 28s - loss: 689.5838 - val_loss: 519.4608
Epoch 775/1000
 - 28s - loss: 689.5842 - val_loss: 519.4515
Epoch 776/1000
 - 27s - loss: 689.5802 - val_loss: 519.4418

Epoch 00776: loss did not improve from 689.57433
Epoch 777/1000
 - 28s - loss: 689.5784 - val_loss: 519.4475
Epoch 778/1000
 - 28s - loss: 689.5770 - val_loss: 519.4398
Epoch 779/1000
 - 28s - loss: 689.5831 - val_loss: 519.4489
Epoch 780/1000
 - 28s - loss: 689.1543 - val_loss: 519.4728

Epoch 00780: loss improved from 689.57433 to 689.15428, saving model to ../Cl_data/Model/Extended/CallbackfullAEP10TT_P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000.hdf5
Epoch 781/1000
 - 28s - loss: 689.1796 - val_loss: 519.4481
Epoch 782/1000
 - 28s - loss: 689.5834 - val_loss: 519.4469
Epoch 783/1000
 - 27s - loss: 689.5831 - val_loss: 519.4415
Epoch 784/1000
 - 27s - loss: 689.5828 - val_loss: 519.4406

Epoch 00784: loss did not improve from 689.15428
Epoch 785/1000
 - 28s - loss: 689.5806 - val_loss: 519.4387
Epoch 786/1000
 - 27s - loss: 689.5799 - val_loss: 519.4402
Epoch 787/1000
 - 27s - loss: 689.5913 - val_loss: 519.5244
Epoch 788/1000
 - 27s - loss: 689.5753 - val_loss: 519.4454

Epoch 00788: loss did not improve from 689.15428
Epoch 789/1000
 - 28s - loss: 689.5840 - val_loss: 519.4461
Epoch 790/1000
 - 28s - loss: 689.6226 - val_loss: 519.4479
Epoch 791/1000
 - 28s - loss: 689.5822 - val_loss: 519.4441
Epoch 792/1000
 - 28s - loss: 689.5710 - val_loss: 519.4422

Epoch 00792: loss did not improve from 689.15428
Epoch 793/1000
 - 28s - loss: 689.5750 - val_loss: 519.4445
Epoch 794/1000
 - 27s - loss: 689.5976 - val_loss: 519.4461
Epoch 795/1000
 - 28s - loss: 689.5742 - val_loss: 519.4510
Epoch 796/1000
 - 28s - loss: 689.5761 - val_loss: 519.4424

Epoch 00796: loss did not improve from 689.15428
Epoch 797/1000
 - 28s - loss: 689.5750 - val_loss: 519.4465
Epoch 798/1000
 - 28s - loss: 689.5735 - val_loss: 519.4480
Epoch 799/1000
 - 28s - loss: 689.6140 - val_loss: 519.4478
Epoch 800/1000
 - 28s - loss: 689.5836 - val_loss: 519.4464

Epoch 00800: loss did not improve from 689.15428
Epoch 801/1000
 - 28s - loss: 689.5806 - val_loss: 519.4468
Epoch 802/1000
 - 28s - loss: 689.5731 - val_loss: 519.4425
Epoch 803/1000
 - 28s - loss: 689.5763 - val_loss: 519.4664
Epoch 804/1000
 - 27s - loss: 689.5781 - val_loss: 519.4486

Epoch 00804: loss did not improve from 689.15428
Epoch 805/1000
 - 27s - loss: 689.5722 - val_loss: 519.4432
Epoch 806/1000
 - 27s - loss: 689.5729 - val_loss: 519.4424
Epoch 807/1000
 - 27s - loss: 689.5704 - val_loss: 519.4439
Epoch 808/1000
 - 28s - loss: 689.5706 - val_loss: 519.4467

Epoch 00808: loss did not improve from 689.15428
Epoch 809/1000
 - 28s - loss: 689.5738 - val_loss: 519.4475
Epoch 810/1000
 - 28s - loss: 689.5735 - val_loss: 519.4736
Epoch 811/1000
 - 28s - loss: 689.5751 - val_loss: 519.4515
Epoch 812/1000
 - 28s - loss: 689.5704 - val_loss: 519.4424

Epoch 00812: loss did not improve from 689.15428
Epoch 813/1000
 - 28s - loss: 689.5698 - val_loss: 519.4434
Epoch 814/1000
 - 27s - loss: 689.5817 - val_loss: 519.4438
Epoch 815/1000
 - 28s - loss: 689.5773 - val_loss: 519.4422
Epoch 816/1000
 - 28s - loss: 689.5718 - val_loss: 519.4435

Epoch 00816: loss did not improve from 689.15428
Epoch 817/1000
 - 28s - loss: 689.5743 - val_loss: 519.4438
Epoch 818/1000
 - 28s - loss: 689.5718 - val_loss: 519.4465
Epoch 819/1000
 - 28s - loss: 689.5757 - val_loss: 519.4412
Epoch 820/1000
 - 28s - loss: 689.5691 - val_loss: 519.4405

Epoch 00820: loss did not improve from 689.15428
Epoch 821/1000
 - 28s - loss: 689.5681 - val_loss: 519.4401
Epoch 822/1000
 - 28s - loss: 689.5702 - val_loss: 519.4415
Epoch 823/1000
 - 28s - loss: 689.5697 - val_loss: 519.4401
Epoch 824/1000
 - 28s - loss: 689.5889 - val_loss: 519.4442

Epoch 00824: loss did not improve from 689.15428
Epoch 825/1000
 - 28s - loss: 691.7509 - val_loss: 519.4440
Epoch 826/1000
 - 28s - loss: 689.5762 - val_loss: 519.4490
Epoch 827/1000
 - 28s - loss: 689.5780 - val_loss: 519.4499
Epoch 828/1000
 - 28s - loss: 689.5717 - val_loss: 519.4427

Epoch 00828: loss did not improve from 689.15428
Epoch 829/1000
 - 28s - loss: 689.5772 - val_loss: 519.4422
Epoch 830/1000
 - 28s - loss: 689.5768 - val_loss: 519.4687
Epoch 831/1000
 - 28s - loss: 689.5927 - val_loss: 519.4512
Epoch 832/1000
 - 28s - loss: 689.5727 - val_loss: 519.4437

Epoch 00832: loss did not improve from 689.15428
Epoch 833/1000
 - 28s - loss: 689.5686 - val_loss: 519.4404
Epoch 834/1000
 - 28s - loss: 689.5808 - val_loss: 519.4536
Epoch 835/1000
 - 28s - loss: 689.5712 - val_loss: 519.4495
Epoch 836/1000
 - 28s - loss: 689.5736 - val_loss: 519.4393

Epoch 00836: loss did not improve from 689.15428
Epoch 837/1000
 - 28s - loss: 689.5725 - val_loss: 519.4499
Epoch 838/1000
 - 28s - loss: 689.5732 - val_loss: 519.4396
Epoch 839/1000
 - 28s - loss: 689.5820 - val_loss: 519.4508
Epoch 840/1000
 - 28s - loss: 689.5853 - val_loss: 519.4742

Epoch 00840: loss did not improve from 689.15428
Epoch 841/1000
 - 28s - loss: 689.5763 - val_loss: 519.4425
Epoch 842/1000
 - 27s - loss: 689.6096 - val_loss: 519.4599
Epoch 843/1000
 - 28s - loss: 689.5702 - val_loss: 519.4388
Epoch 844/1000
 - 27s - loss: 689.5704 - val_loss: 519.4412

Epoch 00844: loss did not improve from 689.15428
Epoch 845/1000
 - 27s - loss: 689.5775 - val_loss: 519.4599
Epoch 846/1000
 - 28s - loss: 689.5709 - val_loss: 519.4405
Epoch 847/1000
 - 28s - loss: 689.5716 - val_loss: 519.4452
Epoch 848/1000
 - 28s - loss: 689.5759 - val_loss: 519.4402

Epoch 00848: loss did not improve from 689.15428
Epoch 849/1000
 - 28s - loss: 689.5702 - val_loss: 519.4653
Epoch 850/1000
 - 28s - loss: 689.5853 - val_loss: 519.4402
Epoch 851/1000
 - 28s - loss: 689.5697 - val_loss: 519.4426
Epoch 852/1000
 - 28s - loss: 689.5740 - val_loss: 519.4471

Epoch 00852: loss did not improve from 689.15428
Epoch 853/1000
 - 28s - loss: 689.5693 - val_loss: 519.4500
Epoch 854/1000
 - 28s - loss: 689.3872 - val_loss: 519.4409
Epoch 855/1000
 - 28s - loss: 689.5720 - val_loss: 519.4428
Epoch 856/1000
 - 28s - loss: 689.5788 - val_loss: 519.4504

Epoch 00856: loss did not improve from 689.15428
Epoch 857/1000
 - 27s - loss: 689.6015 - val_loss: 519.4490
Epoch 858/1000
 - 27s - loss: 689.5776 - val_loss: 519.4805
Epoch 859/1000
 - 27s - loss: 689.6603 - val_loss: 519.5321
Epoch 860/1000
 - 27s - loss: 689.6086 - val_loss: 519.4605

Epoch 00860: loss did not improve from 689.15428
Epoch 861/1000
 - 27s - loss: 689.6071 - val_loss: 519.4491
Epoch 862/1000
 - 27s - loss: 689.5781 - val_loss: 519.4471
Epoch 863/1000
 - 27s - loss: 689.5718 - val_loss: 519.4443
Epoch 864/1000
 - 28s - loss: 689.5834 - val_loss: 519.4523

Epoch 00864: loss did not improve from 689.15428
Epoch 865/1000
 - 28s - loss: 689.3655 - val_loss: 519.4426
Epoch 866/1000
 - 27s - loss: 689.5725 - val_loss: 519.4496
Epoch 867/1000
 - 28s - loss: 689.5698 - val_loss: 519.4460
Epoch 868/1000
 - 28s - loss: 689.5557 - val_loss: 519.3401

Epoch 00868: loss did not improve from 689.15428
Epoch 869/1000
 - 28s - loss: 689.4536 - val_loss: 519.3408
Epoch 870/1000
 - 28s - loss: 689.4499 - val_loss: 519.3347
Epoch 871/1000
 - 28s - loss: 689.4505 - val_loss: 519.3308
Epoch 872/1000
 - 28s - loss: 689.4509 - val_loss: 519.3331

Epoch 00872: loss did not improve from 689.15428
Epoch 873/1000
 - 28s - loss: 689.4510 - val_loss: 519.3327
Epoch 874/1000
 - 28s - loss: 689.4507 - val_loss: 519.3300
Epoch 875/1000
 - 28s - loss: 689.4510 - val_loss: 519.3703
Epoch 876/1000
 - 28s - loss: 689.4514 - val_loss: 519.3363

Epoch 00876: loss did not improve from 689.15428
Epoch 877/1000
 - 28s - loss: 689.4485 - val_loss: 519.3296
Epoch 878/1000
 - 28s - loss: 689.4530 - val_loss: 519.3376
Epoch 879/1000
 - 28s - loss: 689.4513 - val_loss: 519.3328
Epoch 880/1000
 - 28s - loss: 689.4607 - val_loss: 519.3824

Epoch 00880: loss did not improve from 689.15428
Epoch 881/1000
 - 28s - loss: 689.4556 - val_loss: 519.3353
Epoch 882/1000
 - 28s - loss: 689.4494 - val_loss: 519.3381
Epoch 883/1000
 - 27s - loss: 689.4493 - val_loss: 519.3349
Epoch 884/1000
 - 27s - loss: 689.5242 - val_loss: 519.5393

Epoch 00884: loss did not improve from 689.15428
Epoch 885/1000
 - 27s - loss: 689.4651 - val_loss: 519.3434
Epoch 886/1000
 - 27s - loss: 689.4566 - val_loss: 519.3371
Epoch 887/1000
 - 27s - loss: 689.4528 - val_loss: 519.3453
Epoch 888/1000
 - 27s - loss: 689.4530 - val_loss: 519.3346

Epoch 00888: loss did not improve from 689.15428
Epoch 889/1000
 - 28s - loss: 689.4546 - val_loss: 519.3334
Epoch 890/1000
 - 28s - loss: 689.4496 - val_loss: 519.3411
Epoch 891/1000
 - 28s - loss: 689.4509 - val_loss: 519.3628
Epoch 892/1000
 - 28s - loss: 689.4584 - val_loss: 519.3391

Epoch 00892: loss did not improve from 689.15428
Epoch 893/1000
 - 28s - loss: 689.4521 - val_loss: 519.3359
Epoch 894/1000
 - 28s - loss: 689.4489 - val_loss: 519.3375
Epoch 895/1000
 - 28s - loss: 689.4565 - val_loss: 519.3367
Epoch 896/1000
 - 28s - loss: 689.4508 - val_loss: 519.3341

Epoch 00896: loss did not improve from 689.15428
Epoch 897/1000
 - 28s - loss: 689.4525 - val_loss: 519.3375
Epoch 898/1000
 - 28s - loss: 689.4493 - val_loss: 519.3323
Epoch 899/1000
 - 28s - loss: 689.4486 - val_loss: 519.3341
Epoch 900/1000
 - 28s - loss: 689.4578 - val_loss: 519.3370

Epoch 00900: loss did not improve from 689.15428
Epoch 901/1000
 - 27s - loss: 689.4564 - val_loss: 519.3344
Epoch 902/1000
 - 28s - loss: 689.4522 - val_loss: 519.3348
Epoch 903/1000
 - 28s - loss: 689.4501 - val_loss: 519.3379
Epoch 904/1000
 - 28s - loss: 689.4514 - val_loss: 519.3425

Epoch 00904: loss did not improve from 689.15428
Epoch 905/1000
 - 28s - loss: 689.4650 - val_loss: 519.3388
Epoch 906/1000
 - 28s - loss: 689.4639 - val_loss: 519.3421
Epoch 907/1000
 - 28s - loss: 689.4530 - val_loss: 519.3320
Epoch 908/1000
 - 28s - loss: 689.4540 - val_loss: 519.3317

Epoch 00908: loss did not improve from 689.15428
Epoch 909/1000
 - 28s - loss: 689.5283 - val_loss: 519.3332
Epoch 910/1000
 - 27s - loss: 689.4499 - val_loss: 519.3343
Epoch 911/1000
 - 27s - loss: 689.4577 - val_loss: 519.3340
Epoch 912/1000
 - 28s - loss: 689.4569 - val_loss: 519.3326

Epoch 00912: loss did not improve from 689.15428
Epoch 913/1000
 - 28s - loss: 689.4637 - val_loss: 519.3344
Epoch 914/1000
 - 28s - loss: 689.4493 - val_loss: 519.3457
Epoch 915/1000
 - 27s - loss: 689.4511 - val_loss: 519.3640
Epoch 916/1000
 - 28s - loss: 689.4706 - val_loss: 519.3440

Epoch 00916: loss did not improve from 689.15428
Epoch 917/1000
 - 28s - loss: 689.4511 - val_loss: 519.3331
Epoch 918/1000
 - 28s - loss: 689.4414 - val_loss: 519.3310
Epoch 919/1000
 - 28s - loss: 689.4654 - val_loss: 519.3406
Epoch 920/1000
 - 28s - loss: 689.5115 - val_loss: 519.3595

Epoch 00920: loss did not improve from 689.15428
Epoch 921/1000
 - 28s - loss: 689.4571 - val_loss: 519.3380
Epoch 922/1000
 - 28s - loss: 689.4533 - val_loss: 519.3315
Epoch 923/1000
 - 28s - loss: 689.4517 - val_loss: 519.3315
Epoch 924/1000
 - 28s - loss: 689.6382 - val_loss: 519.3330

Epoch 00924: loss did not improve from 689.15428
Epoch 925/1000
 - 28s - loss: 689.4503 - val_loss: 519.3297
Epoch 926/1000
 - 28s - loss: 689.4533 - val_loss: 519.3318
Epoch 927/1000
 - 28s - loss: 689.4566 - val_loss: 519.3348
Epoch 928/1000
 - 28s - loss: 689.4532 - val_loss: 519.3330

Epoch 00928: loss did not improve from 689.15428
Epoch 929/1000
 - 28s - loss: 689.4528 - val_loss: 519.3351
Epoch 930/1000
 - 28s - loss: 689.4644 - val_loss: 519.3407
Epoch 931/1000
 - 28s - loss: 690.9949 - val_loss: 519.3345
Epoch 932/1000
 - 28s - loss: 689.4653 - val_loss: 519.3306

Epoch 00932: loss did not improve from 689.15428
Epoch 933/1000
 - 28s - loss: 689.4572 - val_loss: 519.3340
Epoch 934/1000
 - 28s - loss: 689.4616 - val_loss: 519.3315
Epoch 935/1000
 - 28s - loss: 689.4497 - val_loss: 519.3327
Epoch 936/1000
 - 28s - loss: 689.4561 - val_loss: 519.3313

Epoch 00936: loss did not improve from 689.15428
Epoch 937/1000
 - 28s - loss: 689.4531 - val_loss: 519.3381
Epoch 938/1000
 - 28s - loss: 689.4921 - val_loss: 519.3326
Epoch 939/1000
 - 28s - loss: 689.4532 - val_loss: 519.3423
Epoch 940/1000
 - 28s - loss: 689.4501 - val_loss: 519.3301

Epoch 00940: loss did not improve from 689.15428
Epoch 941/1000
 - 28s - loss: 689.4501 - val_loss: 519.3319
Epoch 942/1000
 - 28s - loss: 689.4557 - val_loss: 519.3352
Epoch 943/1000
 - 28s - loss: 689.4806 - val_loss: 519.3404
Epoch 944/1000
 - 28s - loss: 689.4530 - val_loss: 519.3302

Epoch 00944: loss did not improve from 689.15428
Epoch 945/1000
 - 28s - loss: 689.4499 - val_loss: 519.3313
Epoch 946/1000
 - 28s - loss: 689.4508 - val_loss: 519.3292
Epoch 947/1000
 - 28s - loss: 689.4484 - val_loss: 519.3320
Epoch 948/1000
 - 28s - loss: 689.4487 - val_loss: 519.3325

Epoch 00948: loss did not improve from 689.15428
Epoch 949/1000
 - 28s - loss: 689.4499 - val_loss: 519.3296
Epoch 950/1000
 - 28s - loss: 689.4871 - val_loss: 519.3315
Epoch 951/1000
 - 28s - loss: 689.4554 - val_loss: 519.3311
Epoch 952/1000
 - 28s - loss: 689.5259 - val_loss: 519.3892

Epoch 00952: loss did not improve from 689.15428
Epoch 953/1000
 - 28s - loss: 689.4601 - val_loss: 519.3545
Epoch 954/1000
 - 28s - loss: 689.4594 - val_loss: 519.3349
Epoch 955/1000
 - 28s - loss: 689.4496 - val_loss: 519.3325
Epoch 956/1000
 - 28s - loss: 689.4541 - val_loss: 519.3415

Epoch 00956: loss did not improve from 689.15428
Epoch 957/1000
 - 28s - loss: 689.4512 - val_loss: 519.3318
Epoch 958/1000
 - 28s - loss: 689.4495 - val_loss: 519.3398
Epoch 959/1000
 - 28s - loss: 689.4510 - val_loss: 519.3337
Epoch 960/1000
 - 28s - loss: 689.4493 - val_loss: 519.3308

Epoch 00960: loss did not improve from 689.15428
Epoch 961/1000
 - 28s - loss: 689.4534 - val_loss: 519.3345
Epoch 962/1000
 - 28s - loss: 689.4487 - val_loss: 519.3480
Epoch 963/1000
 - 28s - loss: 689.5170 - val_loss: 519.5154
Epoch 964/1000
 - 28s - loss: 690.7144 - val_loss: 519.3382

Epoch 00964: loss did not improve from 689.15428
Epoch 965/1000
 - 27s - loss: 689.4566 - val_loss: 519.3343
Epoch 966/1000
 - 27s - loss: 691.0371 - val_loss: 519.6590
Epoch 967/1000
 - 28s - loss: 689.0718 - val_loss: 519.3411
Epoch 968/1000
 - 27s - loss: 689.7956 - val_loss: 519.3390

Epoch 00968: loss did not improve from 689.15428
Epoch 969/1000
 - 27s - loss: 689.4857 - val_loss: 519.3372
Epoch 970/1000
 - 28s - loss: 689.4932 - val_loss: 519.3393
Epoch 971/1000
 - 28s - loss: 689.4600 - val_loss: 519.3347
Epoch 972/1000
 - 28s - loss: 689.4532 - val_loss: 519.3349

Epoch 00972: loss did not improve from 689.15428
Epoch 973/1000
 - 28s - loss: 689.4534 - val_loss: 519.3344
Epoch 974/1000
 - 28s - loss: 689.5353 - val_loss: 519.3368
Epoch 975/1000
 - 28s - loss: 689.4533 - val_loss: 519.3335
Epoch 976/1000
 - 28s - loss: 689.4536 - val_loss: 519.3432

Epoch 00976: loss did not improve from 689.15428
Epoch 977/1000
 - 28s - loss: 689.4530 - val_loss: 519.3353
Epoch 978/1000
 - 28s - loss: 689.4518 - val_loss: 519.3320
Epoch 979/1000
 - 28s - loss: 689.4521 - val_loss: 519.3333
Epoch 980/1000
 - 28s - loss: 689.4525 - val_loss: 519.3335

Epoch 00980: loss did not improve from 689.15428
Epoch 981/1000
 - 28s - loss: 689.4530 - val_loss: 519.3380
Epoch 982/1000
 - 28s - loss: 689.4533 - val_loss: 519.3438
Epoch 983/1000
 - 28s - loss: 689.4510 - val_loss: 519.3327
Epoch 984/1000
 - 28s - loss: 689.4521 - val_loss: 519.3430

Epoch 00984: loss did not improve from 689.15428
Epoch 985/1000
 - 28s - loss: 689.4532 - val_loss: 519.3394
Epoch 986/1000
 - 27s - loss: 689.4513 - val_loss: 519.3320
Epoch 987/1000
 - 27s - loss: 689.4535 - val_loss: 519.3372
Epoch 988/1000
 - 28s - loss: 689.4627 - val_loss: 519.3347

Epoch 00988: loss did not improve from 689.15428
Epoch 989/1000
 - 28s - loss: 689.4530 - val_loss: 519.3340
Epoch 990/1000
 - 28s - loss: 689.4524 - val_loss: 519.3315
Epoch 991/1000
 - 28s - loss: 689.4637 - val_loss: 519.3488
Epoch 992/1000
 - 28s - loss: 689.4612 - val_loss: 519.3318

Epoch 00992: loss did not improve from 689.15428
Epoch 993/1000
 - 28s - loss: 689.4546 - val_loss: 519.3401
Epoch 994/1000
 - 28s - loss: 689.4567 - val_loss: 519.3372
Epoch 995/1000
 - 28s - loss: 689.4520 - val_loss: 519.3340
Epoch 996/1000
 - 28s - loss: 689.4584 - val_loss: 519.4712

Epoch 00996: loss did not improve from 689.15428
Epoch 997/1000
 - 28s - loss: 689.5073 - val_loss: 519.3535
Epoch 998/1000
 - 28s - loss: 689.4571 - val_loss: 519.3363
Epoch 999/1000
 - 27s - loss: 689.4509 - val_loss: 519.3331
Epoch 1000/1000
 - 28s - loss: 689.4549 - val_loss: 519.3332
Cl_extendedP10.py:429: RuntimeWarning: divide by zero encountered in true_divide
  plt.plot(ls, x_train_decoded[i]/x_train[i], 'r-', alpha = 0.8)
Cl_extendedP10.py:429: RuntimeWarning: invalid value encountered in true_divide
  plt.plot(ls, x_train_decoded[i]/x_train[i], 'r-', alpha = 0.8)
/homes/nramachandra/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.
  % get_backend())
Cl_extendedP10.py:460: RuntimeWarning: divide by zero encountered in true_divide
  print('--------max ratio (train) : ', np.max(x_train_decoded/x_train) )
Cl_extendedP10.py:460: RuntimeWarning: invalid value encountered in true_divide
  print('--------max ratio (train) : ', np.max(x_train_decoded/x_train) )

Epoch 01000: loss did not improve from 689.15428
--------learning rate :  1e-04
=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~
Time taken ------------------: 462.1059247334798
=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~
Models saved in: ../Cl_data/Model/Extended/
P10Model_tot1024_batch8_lr0.0001_decay0.5_z64_epoch1000
TT
--------max ratio (train) :  nan
--------max ratio (test)  :  2.007919515467836
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
[27736] End job
